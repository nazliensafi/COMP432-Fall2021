{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9b25739",
   "metadata": {},
   "source": [
    "# 3. Classifier interpretability\n",
    "\n",
    "In this section we train two types of models: a decision tree, and a convolutional neural network, to inspect \"which model is more interpretable?\".\n",
    "\n",
    "In the following steps:\n",
    "\n",
    "1. Process the CIFAR-10 dataset.\n",
    "2. Define and train a convolutional neural network (CNN) classifier using PyTorch.\n",
    "3. Interpret the CNN using the 'activation maximization' technique.\n",
    "4. Define and train a decision tree classifier.\n",
    "5. Interpret the decision tree and\n",
    "\n",
    "**Run the code below** to import required packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8fdc4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac12f1a1",
   "metadata": {},
   "source": [
    "Load data\n",
    "\n",
    "Srource: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd74c432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def load_data(root='datasets/'):\n",
    "\n",
    "    # normalize values from [0, 1] to [-1, 1]\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    batch_size = 5\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(root=root, train=True,\n",
    "                                            download=True, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                              shuffle=True, num_workers=2)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root=root, train=False,\n",
    "                                           download=True, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                             shuffle=False, num_workers=2)    \n",
    "\n",
    "    \n",
    "    return trainloader, testloader\n",
    "\n",
    "\n",
    "\n",
    "# Get the train_oader and testloader\n",
    "trainloader, testloader = load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830a1902",
   "metadata": {},
   "source": [
    "### Define methods to read, unpickles the data batches and convert them into np arrays for training and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54f711f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(filepath, filename):\n",
    "    \"\"\"\n",
    "    read the file and return the file contents\n",
    "    \n",
    "    @param filepath: file relative path\n",
    "    @param filename: file name\n",
    "    \"\"\"\n",
    "    with open(os.path.join(filepath, filename), 'rb') as fo:            \n",
    "        file = pickle.load(fo, encoding='bytes')\n",
    "    return file\n",
    "\n",
    "def get_train_test():\n",
    "\n",
    "    # the folder path where it stores the CIFAR-10 training batches files\n",
    "    data_path = 'datasets/cifar-10-batches-py'\n",
    "        \n",
    "    # file names for training batches\n",
    "    training_batches = ('data_batch_1', 'data_batch_2', 'data_batch_3', 'data_batch_4', 'data_batch_5')        \n",
    "       \n",
    "    # temporary variables for reading files\n",
    "    train_data = None\n",
    "    train_labels = None\n",
    "        \n",
    "    for filename in training_batches:\n",
    "        # read the file contents\n",
    "        dict_train = unpickle(data_path, filename)\n",
    "        file_data = dict_train[b'data']\n",
    "        \n",
    "        file_labels = dict_train[b'labels']\n",
    "            \n",
    "        # append the file contents in each batch to train_data and train_labels\n",
    "        if train_data is None and train_labels is None:\n",
    "            # for the first training batch\n",
    "            train_data = file_data\n",
    "            train_labels = file_labels\n",
    "        else:\n",
    "\n",
    "            # for the following training batches\n",
    "            #Stack arrays in sequence vertically (row wise) using vstack\n",
    "            # This is equivalent to concatenation along the \n",
    "            # first axis after 1-D arrays of shape (N,) have been reshaped to (1,N). \n",
    "            train_data = np.vstack((train_data, file_data))\n",
    "            # stack arrays in sequence horizontally (column wise) using hstack\n",
    "            train_labels = np.hstack((train_labels, file_labels))\n",
    "    x_train = train_data      # (50000, 3072)\n",
    "    y_train = train_labels    # (50000,)\n",
    "    \n",
    "    # read test batch file\n",
    "    test_batch = 'test_batch'\n",
    "    \n",
    "    # read the file contents\n",
    "    dict_test = unpickle(data_path, test_batch)\n",
    "        \n",
    "    x_test = dict_test[b'data']      # (10000, 3072)\n",
    "    y_test = dict_test[b'labels']    # (10000,)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_test, y_test = get_train_test();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a97a78c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = get_train_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d333e74",
   "metadata": {},
   "source": [
    "**Run the code cell below** to do Cross validation using GridSearch over the hyper-parameters: criterion and max_depth to find the best Decision Tree classifier:\n",
    "\n",
    "Running the following code cell takes about 9 minues on a Unix platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb188e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a decision tree classifier\n",
    "#To obtain a deterministic behaviour during fitting, random_state has to be fixed to an integer\n",
    "dec_tree = DecisionTreeClassifier(random_state=0)\n",
    "# define parameters range\n",
    "params = {'max_depth': [2,4,6,8], 'criterion': ['gini', 'entropy']}\n",
    "#params = dict(dec_tree__criterion=criterion, dec_tree__max_depth=max_depth)\n",
    "\n",
    "# apply grid search cross validation\n",
    "cv_grid = GridSearchCV(estimator=dec_tree, \n",
    "                       cv=3,    # 3-fold cross-validation\n",
    "                       n_jobs=-1,    # use all processors to train\n",
    "                       param_grid=params)\n",
    "\n",
    "cv_grid.fit(x_train, y_train)\n",
    "\n",
    "# get the best estimator\n",
    "gridcv_best_estimator = cv_grid.best_estimator_\n",
    "\n",
    "# print the results (Accuracy and Recall scores(macro because all classes need to be treated equally))\n",
    "print('Best GridSearch estimator: ', gridcv_best_estimator)\n",
    "\n",
    "print('Accuracy on the 50000 train images: %.2f %%' \n",
    "      % (100 * gridcv_best_estimator.score(X=x_train, y=y_train)))\n",
    "print('Accuracy on the 10000 test images: %.2f %%' \n",
    "      % (100 * gridcv_best_estimator.score(X=x_test, y=y_test)))\n",
    "print(\"Recall (average='macro') on the 50000 train images: %.3f\" \n",
    "      % (recall_score(y_true=y_train, y_pred=gridcv_best_estimator.predict(x_train), average='macro')))\n",
    "print(\"Recall (average='macro') on the 10000 test images: %.3f\" \n",
    "      % (recall_score(y_true=y_test, y_pred=gridcv_best_estimator.predict(x_test), average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b878e3cc",
   "metadata": {},
   "source": [
    "**Run the code cell below** to plot the best tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f7e063",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_depth = gridcv_best_estimator.get_depth()\n",
    "plt.figure(figsize=(20,8), dpi=300)\n",
    "\n",
    "plot_tree(decision_tree=gridcv_best_estimator,\n",
    "        max_depth=2,    # only plot the top plot_depth layers\n",
    "        rotate=True,\n",
    "        fontsize=8)\n",
    "plt.title('decision tree classifier (depth = ' + str(tree_depth) + ')')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731d2f26",
   "metadata": {},
   "source": [
    "**Run the code cell below**, which will take about a minute, to instanciate, train and print the classification report of the 'best' decision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8af11ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate the best decition tree (max_depth=8, criterion='gini')\n",
    "dt_best = DecisionTreeClassifier(max_depth=8, random_state=0, criterion='gini')\n",
    "# train the best decision tree\n",
    "dt_best.fit(x_train, y_train)\n",
    "# print the classification report\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "print(classification_report(y_test, dt_best.predict(x_test), target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6a95cc",
   "metadata": {},
   "source": [
    "### Define Class Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03b4fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2134b55",
   "metadata": {},
   "source": [
    "#### Display a sample image in the proper format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2668b8bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0].reshape((3, 32, 32)).transpose(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8e414c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aff1d4bf",
   "metadata": {},
   "source": [
    "Define, train and save a CNN classifier using any architecture you like, but keep trying until you achieve at least 75% accuracy on the CIFAR test images. Train using CrossEntropyLoss. \n",
    "\n",
    "## Define a Covolutional Neural Network\n",
    "\n",
    "Reference: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd7d16d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 3 in channels (one for each RGB), 6 out channels, and a kernel size of 5.\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        # Get the maximum feature in 2x2 kernel window and down scale the rest.\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # 6 input channels (from previous 6 out), 16 out channels, and a kernel size of 5.\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # input features = 16 output from conv2 * (5x5) kernel size. Outputs 120 features.\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        # input 120 features from fc1, outputs 84 features.\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        # input 84 features from fc2, outputs 10 features.\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # relu activation function converts negative values to 0 on conv1, conv2, fc1 and fc2.\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # flatten input to a 1 d tensor\n",
    "        x = torch.flatten(x, 1) \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e8b0f2",
   "metadata": {},
   "source": [
    "## Define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bdf4395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the loss and optimization functions\n",
    "def loss_function(network, learning_rate=0.001, weight_decay=1e-3):\n",
    "    # Function to compute loss between result and target\n",
    "    cel = nn.CrossEntropyLoss()\n",
    "    #  Stochastic Gradient Descent taking iterator of network object's parameters, ...\n",
    "    # learning rate, and momentum(convergence rate of SGD)\n",
    "\n",
    "    # using a small learning rate, our weights decay gradually with each epoch.\n",
    "    optimizer = optim.SGD(network.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    return cel, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7270cd",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c2dc774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs, amount of times to go over and train the data.\n",
    "def train(file_name, network, criterion, optimizer, epochs = 2, data=trainloader):\n",
    "    # Used to decay the learning rate after every epoch using gamma as a multiplicitive factor.\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9) \n",
    "\n",
    "    # Loop over training for each epoch.\n",
    "    for epoch in range(epochs): \n",
    "        loss_tracker = 0.0\n",
    "        network.train()\n",
    "        for i, images in enumerate(data, 0):\n",
    "            # split 'images' based on list [inputs, labels]\n",
    "            inputs, labels = images\n",
    "\n",
    "            # Set optimizer gradients to 0\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Calls the 'forward' function of network\n",
    "            outputs = network(inputs)\n",
    "            # Calculate the loss from the forward results\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Take the gradient of loss.\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update Model Params.\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            loss_tracker += loss.item()\n",
    "            if i % 1000 == 999:    # print every 1000 mini-batches\n",
    "                print('[Epoch: %d, Data seen: %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, loss_tracker / 1000))\n",
    "                loss_tracker = 0.0\n",
    "        \n",
    "        ## Save the model state after epoch\n",
    "        network.eval() \n",
    "        torch.save(network.state_dict(), file_name+str(epoch+1)+'.pt')\n",
    "        scheduler.step()\n",
    "        \n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf8209f",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6692e22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(network, learning_rate=0.001, epochs=2):\n",
    "    PATH = './TrainedModels/'+ str(learning_rate)+'_'+str(epochs)+'.pth'\n",
    "    torch.save(network.state_dict(), PATH)\n",
    "    print(\"File saved\")\n",
    "    return PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d839da",
   "metadata": {},
   "source": [
    "### Main function to run the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f3be822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the saved path if save=True, else return none.\n",
    "# file_name refers to the prefex for each model epoch to be save to.\n",
    "def run_CNN(file_name, learning_rate=0.001, epochs=2, save=False, data=trainloader, weight_decay=1e-3):\n",
    "    network = Network()\n",
    "    cel, opt = loss_function(network, learning_rate, weight_decay=weight_decay)\n",
    "    train(file_name, network, cel, opt, epochs, data)\n",
    "    if save:\n",
    "        return save_model(network, learning_rate, epochs)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e0421c",
   "metadata": {},
   "source": [
    "### Run the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2642261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1, Data seen:  1000] loss: 2.271\n",
      "[Epoch: 1, Data seen:  2000] loss: 2.058\n",
      "[Epoch: 1, Data seen:  3000] loss: 1.931\n",
      "[Epoch: 1, Data seen:  4000] loss: 1.793\n",
      "[Epoch: 1, Data seen:  5000] loss: 1.695\n",
      "[Epoch: 1, Data seen:  6000] loss: 1.608\n",
      "[Epoch: 1, Data seen:  7000] loss: 1.568\n",
      "[Epoch: 1, Data seen:  8000] loss: 1.529\n",
      "[Epoch: 1, Data seen:  9000] loss: 1.513\n",
      "[Epoch: 1, Data seen: 10000] loss: 1.506\n",
      "[Epoch: 2, Data seen:  1000] loss: 1.434\n",
      "[Epoch: 2, Data seen:  2000] loss: 1.435\n",
      "[Epoch: 2, Data seen:  3000] loss: 1.418\n",
      "[Epoch: 2, Data seen:  4000] loss: 1.383\n",
      "[Epoch: 2, Data seen:  5000] loss: 1.370\n",
      "[Epoch: 2, Data seen:  6000] loss: 1.334\n",
      "[Epoch: 2, Data seen:  7000] loss: 1.350\n",
      "[Epoch: 2, Data seen:  8000] loss: 1.329\n",
      "[Epoch: 2, Data seen:  9000] loss: 1.298\n",
      "[Epoch: 2, Data seen: 10000] loss: 1.287\n",
      "[Epoch: 3, Data seen:  1000] loss: 1.247\n",
      "[Epoch: 3, Data seen:  2000] loss: 1.258\n",
      "[Epoch: 3, Data seen:  3000] loss: 1.213\n",
      "[Epoch: 3, Data seen:  4000] loss: 1.206\n",
      "[Epoch: 3, Data seen:  5000] loss: 1.231\n",
      "[Epoch: 3, Data seen:  6000] loss: 1.216\n",
      "[Epoch: 3, Data seen:  7000] loss: 1.187\n",
      "[Epoch: 3, Data seen:  8000] loss: 1.218\n",
      "[Epoch: 3, Data seen:  9000] loss: 1.193\n",
      "[Epoch: 3, Data seen: 10000] loss: 1.162\n",
      "[Epoch: 4, Data seen:  1000] loss: 1.114\n",
      "[Epoch: 4, Data seen:  2000] loss: 1.097\n",
      "[Epoch: 4, Data seen:  3000] loss: 1.101\n",
      "[Epoch: 4, Data seen:  4000] loss: 1.121\n",
      "[Epoch: 4, Data seen:  5000] loss: 1.130\n",
      "[Epoch: 4, Data seen:  6000] loss: 1.084\n",
      "[Epoch: 4, Data seen:  7000] loss: 1.089\n",
      "[Epoch: 4, Data seen:  8000] loss: 1.127\n",
      "[Epoch: 4, Data seen:  9000] loss: 1.092\n",
      "[Epoch: 4, Data seen: 10000] loss: 1.102\n",
      "[Epoch: 5, Data seen:  1000] loss: 1.015\n",
      "[Epoch: 5, Data seen:  2000] loss: 1.015\n",
      "[Epoch: 5, Data seen:  3000] loss: 1.014\n",
      "[Epoch: 5, Data seen:  4000] loss: 1.034\n",
      "[Epoch: 5, Data seen:  5000] loss: 1.048\n",
      "[Epoch: 5, Data seen:  6000] loss: 1.059\n",
      "[Epoch: 5, Data seen:  7000] loss: 1.015\n",
      "[Epoch: 5, Data seen:  8000] loss: 1.031\n",
      "[Epoch: 5, Data seen:  9000] loss: 1.005\n",
      "[Epoch: 5, Data seen: 10000] loss: 1.052\n",
      "[Epoch: 6, Data seen:  1000] loss: 0.963\n",
      "[Epoch: 6, Data seen:  2000] loss: 0.963\n",
      "[Epoch: 6, Data seen:  3000] loss: 0.951\n",
      "[Epoch: 6, Data seen:  4000] loss: 0.962\n",
      "[Epoch: 6, Data seen:  5000] loss: 0.965\n",
      "[Epoch: 6, Data seen:  6000] loss: 0.973\n",
      "[Epoch: 6, Data seen:  7000] loss: 0.958\n",
      "[Epoch: 6, Data seen:  8000] loss: 0.976\n",
      "[Epoch: 6, Data seen:  9000] loss: 0.957\n",
      "[Epoch: 6, Data seen: 10000] loss: 0.955\n",
      "[Epoch: 7, Data seen:  1000] loss: 0.915\n",
      "[Epoch: 7, Data seen:  2000] loss: 0.879\n",
      "[Epoch: 7, Data seen:  3000] loss: 0.889\n",
      "[Epoch: 7, Data seen:  4000] loss: 0.904\n",
      "[Epoch: 7, Data seen:  5000] loss: 0.902\n",
      "[Epoch: 7, Data seen:  6000] loss: 0.906\n",
      "[Epoch: 7, Data seen:  7000] loss: 0.913\n",
      "[Epoch: 7, Data seen:  8000] loss: 0.903\n",
      "[Epoch: 7, Data seen:  9000] loss: 0.909\n",
      "[Epoch: 7, Data seen: 10000] loss: 0.943\n",
      "[Epoch: 8, Data seen:  1000] loss: 0.848\n",
      "[Epoch: 8, Data seen:  2000] loss: 0.832\n",
      "[Epoch: 8, Data seen:  3000] loss: 0.859\n",
      "[Epoch: 8, Data seen:  4000] loss: 0.838\n",
      "[Epoch: 8, Data seen:  5000] loss: 0.855\n",
      "[Epoch: 8, Data seen:  6000] loss: 0.844\n",
      "[Epoch: 8, Data seen:  7000] loss: 0.861\n",
      "[Epoch: 8, Data seen:  8000] loss: 0.876\n",
      "[Epoch: 8, Data seen:  9000] loss: 0.892\n",
      "[Epoch: 8, Data seen: 10000] loss: 0.865\n",
      "[Epoch: 9, Data seen:  1000] loss: 0.777\n",
      "[Epoch: 9, Data seen:  2000] loss: 0.805\n",
      "[Epoch: 9, Data seen:  3000] loss: 0.791\n",
      "[Epoch: 9, Data seen:  4000] loss: 0.799\n",
      "[Epoch: 9, Data seen:  5000] loss: 0.822\n",
      "[Epoch: 9, Data seen:  6000] loss: 0.806\n",
      "[Epoch: 9, Data seen:  7000] loss: 0.841\n",
      "[Epoch: 9, Data seen:  8000] loss: 0.828\n",
      "[Epoch: 9, Data seen:  9000] loss: 0.813\n",
      "[Epoch: 9, Data seen: 10000] loss: 0.836\n",
      "[Epoch: 10, Data seen:  1000] loss: 0.736\n",
      "[Epoch: 10, Data seen:  2000] loss: 0.783\n",
      "[Epoch: 10, Data seen:  3000] loss: 0.733\n",
      "[Epoch: 10, Data seen:  4000] loss: 0.761\n",
      "[Epoch: 10, Data seen:  5000] loss: 0.773\n",
      "[Epoch: 10, Data seen:  6000] loss: 0.767\n",
      "[Epoch: 10, Data seen:  7000] loss: 0.793\n",
      "[Epoch: 10, Data seen:  8000] loss: 0.779\n",
      "[Epoch: 10, Data seen:  9000] loss: 0.784\n",
      "[Epoch: 10, Data seen: 10000] loss: 0.799\n",
      "[Epoch: 11, Data seen:  1000] loss: 0.721\n",
      "[Epoch: 11, Data seen:  2000] loss: 0.718\n",
      "[Epoch: 11, Data seen:  3000] loss: 0.706\n",
      "[Epoch: 11, Data seen:  4000] loss: 0.735\n",
      "[Epoch: 11, Data seen:  5000] loss: 0.745\n",
      "[Epoch: 11, Data seen:  6000] loss: 0.731\n",
      "[Epoch: 11, Data seen:  7000] loss: 0.746\n",
      "[Epoch: 11, Data seen:  8000] loss: 0.726\n",
      "[Epoch: 11, Data seen:  9000] loss: 0.745\n",
      "[Epoch: 11, Data seen: 10000] loss: 0.759\n",
      "[Epoch: 12, Data seen:  1000] loss: 0.652\n",
      "[Epoch: 12, Data seen:  2000] loss: 0.679\n",
      "[Epoch: 12, Data seen:  3000] loss: 0.690\n",
      "[Epoch: 12, Data seen:  4000] loss: 0.694\n",
      "[Epoch: 12, Data seen:  5000] loss: 0.699\n",
      "[Epoch: 12, Data seen:  6000] loss: 0.707\n",
      "[Epoch: 12, Data seen:  7000] loss: 0.709\n",
      "[Epoch: 12, Data seen:  8000] loss: 0.705\n",
      "[Epoch: 12, Data seen:  9000] loss: 0.732\n",
      "[Epoch: 12, Data seen: 10000] loss: 0.718\n"
     ]
    }
   ],
   "source": [
    "model_path = run_CNN('TrainedModels/model_', epochs=30, learning_rate=0.01, weight_decay=1e-3, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be897fd6",
   "metadata": {},
   "source": [
    "### Print method for a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "74ba22bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    # Reverse normalization\n",
    "    img = img / 2 + 0.5    \n",
    "    # set tensor to numpy array\n",
    "    img = img.numpy()\n",
    "    # set channels to proper formal\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fbb76d",
   "metadata": {},
   "source": [
    "## Evaluate the CNN above on the test images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256a11d2",
   "metadata": {},
   "source": [
    "### Method to Graph the accuracy of the model over each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "84a7d329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs to take into account\n",
    "def accuracy_on_epoch(epoch, title, augmented=False ):    \n",
    "    accuracys = []\n",
    "    base_path = './TrainedModels/model_'\n",
    "    if augmented:\n",
    "        base_path = './TrainedModels/augmented_model_'\n",
    "    for i in range(epoch):\n",
    "        loaded_network = Network()\n",
    "        loaded_network.load_state_dict(torch.load(base_path+str(i+1)+'.pt'))\n",
    "        # Trackers for functions.\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Compare true values to model prediction.\n",
    "        with torch.no_grad():\n",
    "            # Taking batches\n",
    "            for data in testloader:\n",
    "                images, labels = data\n",
    "                outputs = loaded_network(images)\n",
    "                # Take highest class score as prediction\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            accuracys.append(correct / total)\n",
    "    # Plot results        \n",
    "    epochs = [i+1 for i in range(epoch)]            \n",
    "    plt.plot(epochs, accuracys)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b57a4e",
   "metadata": {},
   "source": [
    "### Evaluate Model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "66057043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(title, epoch, augmented=False):\n",
    "    # Classes Trackers for scoring\n",
    "    # correct refers to true poitive guess for a class \n",
    "    correct = {class_name: 0 for class_name in class_names}\n",
    "    # wrong refers to a false positive guess for a class\n",
    "    wrong = {class_name: 0 for class_name in class_names}\n",
    "    # True count for a class\n",
    "    total = {class_name: 0 for class_name in class_names}\n",
    "\n",
    "    base_path = './TrainedModels/model_'\n",
    "    if augmented:\n",
    "        base_path = './TrainedModels/augmented_model_'\n",
    "    loaded_network = Network()\n",
    "    loaded_network.load_state_dict(torch.load(base_path+str(epoch)+'.pt'))\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = loaded_network(images)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "            # count the correct predicitons for each class \n",
    "            for label, prediction in zip(labels, predictions):\n",
    "                if label == prediction:\n",
    "                    correct[class_names[label]] += 1\n",
    "                else: wrong[class_names[label]] += 1\n",
    "                total[class_names[label]] += 1\n",
    "\n",
    "\n",
    "    # Print results for each class \n",
    "    for class_name, correct_count in correct.items():\n",
    "        \n",
    "        # Calculate Precision, Recall and F-score\n",
    "        precision = 100 * correct_count/(correct_count + wrong[class_name])\n",
    "        recall = 100 * correct_count/(correct_count +  total[class_name])\n",
    "        f_score = 2*precision*recall/(precision + recall)\n",
    "        print(\"Class {:5s}  Precision: {:.2f} % Recall: {:.2f} % F-Score: {:.2f}\".format(class_name, precision, recall, f_score))\n",
    "\n",
    "    \n",
    "    print('\\nAccuracy: {:.2f} %'.format(100 * sum([correct[i] for i in correct]) / sum([total[i] for i in total])))\n",
    "\n",
    "\n",
    "    accuracy_on_epoch(epoch, title, augmented=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eeb4c545",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(\"Accuracy per epoch\", 30, augmented=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b208947",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "06687c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new data based on mirrored versions of each image, rotated images, and mirrored + rotated sets.\n",
    "new_data = []\n",
    "for i, data in enumerate(trainloader):\n",
    "    images, labels = data\n",
    "    flip_1 = torch.rot90(images, k=1, dims=(2,3))\n",
    "    new_data.append([images, labels])\n",
    "    mirror = torch.flip(images, [3])\n",
    "    mirror2 = torch.flip(flip_1, [3])\n",
    "    new_data.append([mirror, labels])\n",
    "    new_data.append([flip_1, labels])\n",
    "    new_data.append([mirror2, labels])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1517fc24",
   "metadata": {},
   "source": [
    "### Train the Model using the augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "603ff74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1, Data seen:  1000] loss: 2.293\n",
      "[Epoch: 1, Data seen:  2000] loss: 2.102\n",
      "[Epoch: 1, Data seen:  3000] loss: 1.945\n",
      "[Epoch: 1, Data seen:  4000] loss: 1.770\n",
      "[Epoch: 1, Data seen:  5000] loss: 1.647\n",
      "[Epoch: 1, Data seen:  6000] loss: 1.702\n",
      "[Epoch: 1, Data seen:  7000] loss: 1.664\n",
      "[Epoch: 1, Data seen:  8000] loss: 1.660\n",
      "[Epoch: 1, Data seen:  9000] loss: 1.633\n",
      "[Epoch: 1, Data seen: 10000] loss: 1.612\n",
      "[Epoch: 1, Data seen: 11000] loss: 1.578\n",
      "[Epoch: 1, Data seen: 12000] loss: 1.539\n",
      "[Epoch: 1, Data seen: 13000] loss: 1.526\n",
      "[Epoch: 1, Data seen: 14000] loss: 1.483\n",
      "[Epoch: 1, Data seen: 15000] loss: 1.483\n",
      "[Epoch: 1, Data seen: 16000] loss: 1.482\n",
      "[Epoch: 1, Data seen: 17000] loss: 1.523\n",
      "[Epoch: 1, Data seen: 18000] loss: 1.497\n",
      "[Epoch: 1, Data seen: 19000] loss: 1.438\n",
      "[Epoch: 1, Data seen: 20000] loss: 1.410\n",
      "[Epoch: 1, Data seen: 21000] loss: 1.421\n",
      "[Epoch: 1, Data seen: 22000] loss: 1.329\n",
      "[Epoch: 1, Data seen: 23000] loss: 1.374\n",
      "[Epoch: 1, Data seen: 24000] loss: 1.350\n",
      "[Epoch: 1, Data seen: 25000] loss: 1.383\n",
      "[Epoch: 1, Data seen: 26000] loss: 1.402\n",
      "[Epoch: 1, Data seen: 27000] loss: 1.338\n",
      "[Epoch: 1, Data seen: 28000] loss: 1.353\n",
      "[Epoch: 1, Data seen: 29000] loss: 1.380\n",
      "[Epoch: 1, Data seen: 30000] loss: 1.281\n",
      "[Epoch: 1, Data seen: 31000] loss: 1.330\n",
      "[Epoch: 1, Data seen: 32000] loss: 1.338\n",
      "[Epoch: 1, Data seen: 33000] loss: 1.284\n",
      "[Epoch: 1, Data seen: 34000] loss: 1.280\n",
      "[Epoch: 1, Data seen: 35000] loss: 1.291\n",
      "[Epoch: 1, Data seen: 36000] loss: 1.263\n",
      "[Epoch: 1, Data seen: 37000] loss: 1.296\n",
      "[Epoch: 1, Data seen: 38000] loss: 1.261\n",
      "[Epoch: 1, Data seen: 39000] loss: 1.271\n",
      "[Epoch: 1, Data seen: 40000] loss: 1.253\n",
      "[Epoch: 2, Data seen:  1000] loss: 1.267\n",
      "[Epoch: 2, Data seen:  2000] loss: 1.296\n",
      "[Epoch: 2, Data seen:  3000] loss: 1.286\n",
      "[Epoch: 2, Data seen:  4000] loss: 1.291\n",
      "[Epoch: 2, Data seen:  5000] loss: 1.186\n",
      "[Epoch: 2, Data seen:  6000] loss: 1.272\n",
      "[Epoch: 2, Data seen:  7000] loss: 1.230\n",
      "[Epoch: 2, Data seen:  8000] loss: 1.283\n",
      "[Epoch: 2, Data seen:  9000] loss: 1.255\n",
      "[Epoch: 2, Data seen: 10000] loss: 1.267\n",
      "[Epoch: 2, Data seen: 11000] loss: 1.232\n",
      "[Epoch: 2, Data seen: 12000] loss: 1.221\n",
      "[Epoch: 2, Data seen: 13000] loss: 1.243\n",
      "[Epoch: 2, Data seen: 14000] loss: 1.164\n",
      "[Epoch: 2, Data seen: 15000] loss: 1.205\n",
      "[Epoch: 2, Data seen: 16000] loss: 1.215\n",
      "[Epoch: 2, Data seen: 17000] loss: 1.264\n",
      "[Epoch: 2, Data seen: 18000] loss: 1.212\n",
      "[Epoch: 2, Data seen: 19000] loss: 1.197\n",
      "[Epoch: 2, Data seen: 20000] loss: 1.201\n",
      "[Epoch: 2, Data seen: 21000] loss: 1.209\n",
      "[Epoch: 2, Data seen: 22000] loss: 1.129\n",
      "[Epoch: 2, Data seen: 23000] loss: 1.183\n",
      "[Epoch: 2, Data seen: 24000] loss: 1.174\n",
      "[Epoch: 2, Data seen: 25000] loss: 1.210\n",
      "[Epoch: 2, Data seen: 26000] loss: 1.215\n",
      "[Epoch: 2, Data seen: 27000] loss: 1.180\n",
      "[Epoch: 2, Data seen: 28000] loss: 1.200\n",
      "[Epoch: 2, Data seen: 29000] loss: 1.217\n",
      "[Epoch: 2, Data seen: 30000] loss: 1.120\n",
      "[Epoch: 2, Data seen: 31000] loss: 1.199\n",
      "[Epoch: 2, Data seen: 32000] loss: 1.133\n",
      "[Epoch: 2, Data seen: 33000] loss: 1.116\n",
      "[Epoch: 2, Data seen: 34000] loss: 1.130\n",
      "[Epoch: 2, Data seen: 35000] loss: 1.173\n",
      "[Epoch: 2, Data seen: 36000] loss: 1.125\n",
      "[Epoch: 2, Data seen: 37000] loss: 1.168\n",
      "[Epoch: 2, Data seen: 38000] loss: 1.165\n",
      "[Epoch: 2, Data seen: 39000] loss: 1.146\n",
      "[Epoch: 2, Data seen: 40000] loss: 1.138\n",
      "[Epoch: 3, Data seen:  1000] loss: 1.140\n",
      "[Epoch: 3, Data seen:  2000] loss: 1.126\n",
      "[Epoch: 3, Data seen:  3000] loss: 1.165\n",
      "[Epoch: 3, Data seen:  4000] loss: 1.163\n",
      "[Epoch: 3, Data seen:  5000] loss: 1.082\n",
      "[Epoch: 3, Data seen:  6000] loss: 1.150\n",
      "[Epoch: 3, Data seen:  7000] loss: 1.123\n",
      "[Epoch: 3, Data seen:  8000] loss: 1.172\n",
      "[Epoch: 3, Data seen:  9000] loss: 1.155\n",
      "[Epoch: 3, Data seen: 10000] loss: 1.157\n",
      "[Epoch: 3, Data seen: 11000] loss: 1.115\n",
      "[Epoch: 3, Data seen: 12000] loss: 1.126\n",
      "[Epoch: 3, Data seen: 13000] loss: 1.132\n",
      "[Epoch: 3, Data seen: 14000] loss: 1.060\n",
      "[Epoch: 3, Data seen: 15000] loss: 1.119\n",
      "[Epoch: 3, Data seen: 16000] loss: 1.118\n",
      "[Epoch: 3, Data seen: 17000] loss: 1.170\n",
      "[Epoch: 3, Data seen: 18000] loss: 1.149\n",
      "[Epoch: 3, Data seen: 19000] loss: 1.132\n",
      "[Epoch: 3, Data seen: 20000] loss: 1.106\n",
      "[Epoch: 3, Data seen: 21000] loss: 1.117\n",
      "[Epoch: 3, Data seen: 22000] loss: 1.064\n",
      "[Epoch: 3, Data seen: 23000] loss: 1.114\n",
      "[Epoch: 3, Data seen: 24000] loss: 1.095\n",
      "[Epoch: 3, Data seen: 25000] loss: 1.130\n",
      "[Epoch: 3, Data seen: 26000] loss: 1.144\n",
      "[Epoch: 3, Data seen: 27000] loss: 1.112\n",
      "[Epoch: 3, Data seen: 28000] loss: 1.109\n",
      "[Epoch: 3, Data seen: 29000] loss: 1.130\n",
      "[Epoch: 3, Data seen: 30000] loss: 1.067\n",
      "[Epoch: 3, Data seen: 31000] loss: 1.109\n",
      "[Epoch: 3, Data seen: 32000] loss: 1.082\n",
      "[Epoch: 3, Data seen: 33000] loss: 1.046\n",
      "[Epoch: 3, Data seen: 34000] loss: 1.075\n",
      "[Epoch: 3, Data seen: 35000] loss: 1.111\n",
      "[Epoch: 3, Data seen: 36000] loss: 1.066\n",
      "[Epoch: 3, Data seen: 37000] loss: 1.092\n",
      "[Epoch: 3, Data seen: 38000] loss: 1.079\n",
      "[Epoch: 3, Data seen: 39000] loss: 1.086\n",
      "[Epoch: 3, Data seen: 40000] loss: 1.074\n",
      "[Epoch: 4, Data seen:  1000] loss: 1.087\n",
      "[Epoch: 4, Data seen:  2000] loss: 1.051\n",
      "[Epoch: 4, Data seen:  3000] loss: 1.084\n",
      "[Epoch: 4, Data seen:  4000] loss: 1.102\n",
      "[Epoch: 4, Data seen:  5000] loss: 1.029\n",
      "[Epoch: 4, Data seen:  6000] loss: 1.076\n",
      "[Epoch: 4, Data seen:  7000] loss: 1.063\n",
      "[Epoch: 4, Data seen:  8000] loss: 1.118\n",
      "[Epoch: 4, Data seen:  9000] loss: 1.113\n",
      "[Epoch: 4, Data seen: 10000] loss: 1.108\n",
      "[Epoch: 4, Data seen: 11000] loss: 1.067\n",
      "[Epoch: 4, Data seen: 12000] loss: 1.059\n",
      "[Epoch: 4, Data seen: 13000] loss: 1.061\n",
      "[Epoch: 4, Data seen: 14000] loss: 1.010\n",
      "[Epoch: 4, Data seen: 15000] loss: 1.057\n",
      "[Epoch: 4, Data seen: 16000] loss: 1.062\n",
      "[Epoch: 4, Data seen: 17000] loss: 1.101\n",
      "[Epoch: 4, Data seen: 18000] loss: 1.091\n",
      "[Epoch: 4, Data seen: 19000] loss: 1.064\n",
      "[Epoch: 4, Data seen: 20000] loss: 1.071\n",
      "[Epoch: 4, Data seen: 21000] loss: 1.065\n",
      "[Epoch: 4, Data seen: 22000] loss: 0.986\n",
      "[Epoch: 4, Data seen: 23000] loss: 1.068\n",
      "[Epoch: 4, Data seen: 24000] loss: 1.022\n",
      "[Epoch: 4, Data seen: 25000] loss: 1.072\n",
      "[Epoch: 4, Data seen: 26000] loss: 1.085\n",
      "[Epoch: 4, Data seen: 27000] loss: 1.054\n",
      "[Epoch: 4, Data seen: 28000] loss: 1.059\n",
      "[Epoch: 4, Data seen: 29000] loss: 1.069\n",
      "[Epoch: 4, Data seen: 30000] loss: 1.009\n",
      "[Epoch: 4, Data seen: 31000] loss: 1.061\n",
      "[Epoch: 4, Data seen: 32000] loss: 1.046\n",
      "[Epoch: 4, Data seen: 33000] loss: 1.024\n",
      "[Epoch: 4, Data seen: 34000] loss: 1.017\n",
      "[Epoch: 4, Data seen: 35000] loss: 1.054\n",
      "[Epoch: 4, Data seen: 36000] loss: 1.045\n",
      "[Epoch: 4, Data seen: 37000] loss: 1.039\n",
      "[Epoch: 4, Data seen: 38000] loss: 1.046\n",
      "[Epoch: 4, Data seen: 39000] loss: 1.045\n",
      "[Epoch: 4, Data seen: 40000] loss: 1.020\n",
      "[Epoch: 5, Data seen:  1000] loss: 1.045\n",
      "[Epoch: 5, Data seen:  2000] loss: 1.020\n",
      "[Epoch: 5, Data seen:  3000] loss: 1.029\n",
      "[Epoch: 5, Data seen:  4000] loss: 1.061\n",
      "[Epoch: 5, Data seen:  5000] loss: 0.973\n",
      "[Epoch: 5, Data seen:  6000] loss: 1.025\n",
      "[Epoch: 5, Data seen:  7000] loss: 1.020\n",
      "[Epoch: 5, Data seen:  8000] loss: 1.078\n",
      "[Epoch: 5, Data seen:  9000] loss: 1.067\n",
      "[Epoch: 5, Data seen: 10000] loss: 1.059\n",
      "[Epoch: 5, Data seen: 11000] loss: 1.027\n",
      "[Epoch: 5, Data seen: 12000] loss: 1.024\n",
      "[Epoch: 5, Data seen: 13000] loss: 1.047\n",
      "[Epoch: 5, Data seen: 14000] loss: 0.984\n",
      "[Epoch: 5, Data seen: 15000] loss: 1.030\n",
      "[Epoch: 5, Data seen: 16000] loss: 1.019\n",
      "[Epoch: 5, Data seen: 17000] loss: 1.062\n",
      "[Epoch: 5, Data seen: 18000] loss: 1.056\n",
      "[Epoch: 5, Data seen: 19000] loss: 1.033\n",
      "[Epoch: 5, Data seen: 20000] loss: 1.011\n",
      "[Epoch: 5, Data seen: 21000] loss: 1.006\n",
      "[Epoch: 5, Data seen: 22000] loss: 0.944\n",
      "[Epoch: 5, Data seen: 23000] loss: 1.026\n",
      "[Epoch: 5, Data seen: 24000] loss: 0.997\n",
      "[Epoch: 5, Data seen: 25000] loss: 1.033\n",
      "[Epoch: 5, Data seen: 26000] loss: 1.043\n",
      "[Epoch: 5, Data seen: 27000] loss: 1.008\n",
      "[Epoch: 5, Data seen: 28000] loss: 1.011\n",
      "[Epoch: 5, Data seen: 29000] loss: 1.030\n",
      "[Epoch: 5, Data seen: 30000] loss: 0.978\n",
      "[Epoch: 5, Data seen: 31000] loss: 1.047\n",
      "[Epoch: 5, Data seen: 32000] loss: 1.016\n",
      "[Epoch: 5, Data seen: 33000] loss: 0.989\n",
      "[Epoch: 5, Data seen: 34000] loss: 0.978\n",
      "[Epoch: 5, Data seen: 35000] loss: 1.008\n",
      "[Epoch: 5, Data seen: 36000] loss: 1.016\n",
      "[Epoch: 5, Data seen: 37000] loss: 0.999\n",
      "[Epoch: 5, Data seen: 38000] loss: 1.017\n",
      "[Epoch: 5, Data seen: 39000] loss: 1.009\n",
      "[Epoch: 5, Data seen: 40000] loss: 0.985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 6, Data seen:  1000] loss: 1.020\n",
      "[Epoch: 6, Data seen:  2000] loss: 0.993\n",
      "[Epoch: 6, Data seen:  3000] loss: 1.013\n",
      "[Epoch: 6, Data seen:  4000] loss: 1.033\n",
      "[Epoch: 6, Data seen:  5000] loss: 0.948\n",
      "[Epoch: 6, Data seen:  6000] loss: 0.995\n",
      "[Epoch: 6, Data seen:  7000] loss: 0.990\n",
      "[Epoch: 6, Data seen:  8000] loss: 1.031\n",
      "[Epoch: 6, Data seen:  9000] loss: 1.030\n",
      "[Epoch: 6, Data seen: 10000] loss: 1.024\n",
      "[Epoch: 6, Data seen: 11000] loss: 0.993\n",
      "[Epoch: 6, Data seen: 12000] loss: 0.981\n",
      "[Epoch: 6, Data seen: 13000] loss: 1.001\n",
      "[Epoch: 6, Data seen: 14000] loss: 0.949\n",
      "[Epoch: 6, Data seen: 15000] loss: 0.992\n",
      "[Epoch: 6, Data seen: 16000] loss: 0.994\n",
      "[Epoch: 6, Data seen: 17000] loss: 1.028\n",
      "[Epoch: 6, Data seen: 18000] loss: 1.021\n",
      "[Epoch: 6, Data seen: 19000] loss: 1.007\n",
      "[Epoch: 6, Data seen: 20000] loss: 0.978\n",
      "[Epoch: 6, Data seen: 21000] loss: 0.975\n",
      "[Epoch: 6, Data seen: 22000] loss: 0.925\n",
      "[Epoch: 6, Data seen: 23000] loss: 0.996\n",
      "[Epoch: 6, Data seen: 24000] loss: 0.971\n",
      "[Epoch: 6, Data seen: 25000] loss: 0.991\n",
      "[Epoch: 6, Data seen: 26000] loss: 1.018\n",
      "[Epoch: 6, Data seen: 27000] loss: 0.972\n",
      "[Epoch: 6, Data seen: 28000] loss: 0.976\n",
      "[Epoch: 6, Data seen: 29000] loss: 0.990\n",
      "[Epoch: 6, Data seen: 30000] loss: 0.961\n",
      "[Epoch: 6, Data seen: 31000] loss: 1.009\n",
      "[Epoch: 6, Data seen: 32000] loss: 0.994\n",
      "[Epoch: 6, Data seen: 33000] loss: 0.967\n",
      "[Epoch: 6, Data seen: 34000] loss: 0.951\n",
      "[Epoch: 6, Data seen: 35000] loss: 0.987\n",
      "[Epoch: 6, Data seen: 36000] loss: 0.996\n",
      "[Epoch: 6, Data seen: 37000] loss: 0.960\n",
      "[Epoch: 6, Data seen: 38000] loss: 0.984\n",
      "[Epoch: 6, Data seen: 39000] loss: 0.976\n",
      "[Epoch: 6, Data seen: 40000] loss: 0.966\n",
      "[Epoch: 7, Data seen:  1000] loss: 0.998\n",
      "[Epoch: 7, Data seen:  2000] loss: 0.956\n",
      "[Epoch: 7, Data seen:  3000] loss: 0.977\n",
      "[Epoch: 7, Data seen:  4000] loss: 1.009\n",
      "[Epoch: 7, Data seen:  5000] loss: 0.925\n",
      "[Epoch: 7, Data seen:  6000] loss: 0.974\n",
      "[Epoch: 7, Data seen:  7000] loss: 0.970\n",
      "[Epoch: 7, Data seen:  8000] loss: 1.002\n",
      "[Epoch: 7, Data seen:  9000] loss: 1.005\n",
      "[Epoch: 7, Data seen: 10000] loss: 0.981\n",
      "[Epoch: 7, Data seen: 11000] loss: 0.960\n",
      "[Epoch: 7, Data seen: 12000] loss: 0.956\n",
      "[Epoch: 7, Data seen: 13000] loss: 0.974\n",
      "[Epoch: 7, Data seen: 14000] loss: 0.928\n",
      "[Epoch: 7, Data seen: 15000] loss: 0.958\n",
      "[Epoch: 7, Data seen: 16000] loss: 0.962\n",
      "[Epoch: 7, Data seen: 17000] loss: 0.995\n",
      "[Epoch: 7, Data seen: 18000] loss: 0.982\n",
      "[Epoch: 7, Data seen: 19000] loss: 0.985\n",
      "[Epoch: 7, Data seen: 20000] loss: 0.948\n",
      "[Epoch: 7, Data seen: 21000] loss: 0.950\n",
      "[Epoch: 7, Data seen: 22000] loss: 0.891\n",
      "[Epoch: 7, Data seen: 23000] loss: 0.962\n",
      "[Epoch: 7, Data seen: 24000] loss: 0.941\n",
      "[Epoch: 7, Data seen: 25000] loss: 0.967\n",
      "[Epoch: 7, Data seen: 26000] loss: 0.998\n",
      "[Epoch: 7, Data seen: 27000] loss: 0.957\n",
      "[Epoch: 7, Data seen: 28000] loss: 0.963\n",
      "[Epoch: 7, Data seen: 29000] loss: 0.979\n",
      "[Epoch: 7, Data seen: 30000] loss: 0.930\n",
      "[Epoch: 7, Data seen: 31000] loss: 0.975\n",
      "[Epoch: 7, Data seen: 32000] loss: 0.962\n",
      "[Epoch: 7, Data seen: 33000] loss: 0.945\n",
      "[Epoch: 7, Data seen: 34000] loss: 0.924\n",
      "[Epoch: 7, Data seen: 35000] loss: 0.962\n",
      "[Epoch: 7, Data seen: 36000] loss: 0.973\n",
      "[Epoch: 7, Data seen: 37000] loss: 0.949\n",
      "[Epoch: 7, Data seen: 38000] loss: 0.968\n",
      "[Epoch: 7, Data seen: 39000] loss: 0.961\n",
      "[Epoch: 7, Data seen: 40000] loss: 0.942\n",
      "[Epoch: 8, Data seen:  1000] loss: 0.960\n",
      "[Epoch: 8, Data seen:  2000] loss: 0.934\n",
      "[Epoch: 8, Data seen:  3000] loss: 0.956\n",
      "[Epoch: 8, Data seen:  4000] loss: 1.001\n",
      "[Epoch: 8, Data seen:  5000] loss: 0.909\n",
      "[Epoch: 8, Data seen:  6000] loss: 0.954\n",
      "[Epoch: 8, Data seen:  7000] loss: 0.951\n",
      "[Epoch: 8, Data seen:  8000] loss: 0.978\n",
      "[Epoch: 8, Data seen:  9000] loss: 0.976\n",
      "[Epoch: 8, Data seen: 10000] loss: 0.959\n",
      "[Epoch: 8, Data seen: 11000] loss: 0.926\n",
      "[Epoch: 8, Data seen: 12000] loss: 0.928\n",
      "[Epoch: 8, Data seen: 13000] loss: 0.950\n",
      "[Epoch: 8, Data seen: 14000] loss: 0.918\n",
      "[Epoch: 8, Data seen: 15000] loss: 0.932\n",
      "[Epoch: 8, Data seen: 16000] loss: 0.941\n",
      "[Epoch: 8, Data seen: 17000] loss: 0.985\n",
      "[Epoch: 8, Data seen: 18000] loss: 0.963\n",
      "[Epoch: 8, Data seen: 19000] loss: 0.955\n",
      "[Epoch: 8, Data seen: 20000] loss: 0.935\n",
      "[Epoch: 8, Data seen: 21000] loss: 0.920\n",
      "[Epoch: 8, Data seen: 22000] loss: 0.863\n",
      "[Epoch: 8, Data seen: 23000] loss: 0.933\n",
      "[Epoch: 8, Data seen: 24000] loss: 0.911\n",
      "[Epoch: 8, Data seen: 25000] loss: 0.942\n",
      "[Epoch: 8, Data seen: 26000] loss: 0.982\n",
      "[Epoch: 8, Data seen: 27000] loss: 0.941\n",
      "[Epoch: 8, Data seen: 28000] loss: 0.935\n",
      "[Epoch: 8, Data seen: 29000] loss: 0.958\n",
      "[Epoch: 8, Data seen: 30000] loss: 0.908\n",
      "[Epoch: 8, Data seen: 31000] loss: 0.953\n",
      "[Epoch: 8, Data seen: 32000] loss: 0.924\n",
      "[Epoch: 8, Data seen: 33000] loss: 0.934\n",
      "[Epoch: 8, Data seen: 34000] loss: 0.904\n",
      "[Epoch: 8, Data seen: 35000] loss: 0.938\n",
      "[Epoch: 8, Data seen: 36000] loss: 0.962\n",
      "[Epoch: 8, Data seen: 37000] loss: 0.936\n",
      "[Epoch: 8, Data seen: 38000] loss: 0.963\n",
      "[Epoch: 8, Data seen: 39000] loss: 0.951\n",
      "[Epoch: 8, Data seen: 40000] loss: 0.927\n",
      "[Epoch: 9, Data seen:  1000] loss: 0.946\n",
      "[Epoch: 9, Data seen:  2000] loss: 0.924\n",
      "[Epoch: 9, Data seen:  3000] loss: 0.943\n",
      "[Epoch: 9, Data seen:  4000] loss: 0.979\n",
      "[Epoch: 9, Data seen:  5000] loss: 0.887\n",
      "[Epoch: 9, Data seen:  6000] loss: 0.936\n",
      "[Epoch: 9, Data seen:  7000] loss: 0.924\n",
      "[Epoch: 9, Data seen:  8000] loss: 0.963\n",
      "[Epoch: 9, Data seen:  9000] loss: 0.959\n",
      "[Epoch: 9, Data seen: 10000] loss: 0.944\n",
      "[Epoch: 9, Data seen: 11000] loss: 0.908\n",
      "[Epoch: 9, Data seen: 12000] loss: 0.920\n",
      "[Epoch: 9, Data seen: 13000] loss: 0.921\n",
      "[Epoch: 9, Data seen: 14000] loss: 0.902\n",
      "[Epoch: 9, Data seen: 15000] loss: 0.919\n",
      "[Epoch: 9, Data seen: 16000] loss: 0.920\n",
      "[Epoch: 9, Data seen: 17000] loss: 0.961\n",
      "[Epoch: 9, Data seen: 18000] loss: 0.938\n",
      "[Epoch: 9, Data seen: 19000] loss: 0.939\n",
      "[Epoch: 9, Data seen: 20000] loss: 0.923\n",
      "[Epoch: 9, Data seen: 21000] loss: 0.901\n",
      "[Epoch: 9, Data seen: 22000] loss: 0.849\n",
      "[Epoch: 9, Data seen: 23000] loss: 0.913\n",
      "[Epoch: 9, Data seen: 24000] loss: 0.897\n",
      "[Epoch: 9, Data seen: 25000] loss: 0.921\n",
      "[Epoch: 9, Data seen: 26000] loss: 0.960\n",
      "[Epoch: 9, Data seen: 27000] loss: 0.921\n",
      "[Epoch: 9, Data seen: 28000] loss: 0.917\n",
      "[Epoch: 9, Data seen: 29000] loss: 0.944\n",
      "[Epoch: 9, Data seen: 30000] loss: 0.887\n",
      "[Epoch: 9, Data seen: 31000] loss: 0.924\n",
      "[Epoch: 9, Data seen: 32000] loss: 0.909\n",
      "[Epoch: 9, Data seen: 33000] loss: 0.915\n",
      "[Epoch: 9, Data seen: 34000] loss: 0.874\n",
      "[Epoch: 9, Data seen: 35000] loss: 0.928\n",
      "[Epoch: 9, Data seen: 36000] loss: 0.947\n",
      "[Epoch: 9, Data seen: 37000] loss: 0.922\n",
      "[Epoch: 9, Data seen: 38000] loss: 0.937\n",
      "[Epoch: 9, Data seen: 39000] loss: 0.932\n",
      "[Epoch: 9, Data seen: 40000] loss: 0.906\n",
      "[Epoch: 10, Data seen:  1000] loss: 0.930\n",
      "[Epoch: 10, Data seen:  2000] loss: 0.899\n",
      "[Epoch: 10, Data seen:  3000] loss: 0.932\n",
      "[Epoch: 10, Data seen:  4000] loss: 0.953\n",
      "[Epoch: 10, Data seen:  5000] loss: 0.871\n",
      "[Epoch: 10, Data seen:  6000] loss: 0.920\n",
      "[Epoch: 10, Data seen:  7000] loss: 0.915\n",
      "[Epoch: 10, Data seen:  8000] loss: 0.961\n",
      "[Epoch: 10, Data seen:  9000] loss: 0.929\n",
      "[Epoch: 10, Data seen: 10000] loss: 0.932\n",
      "[Epoch: 10, Data seen: 11000] loss: 0.886\n",
      "[Epoch: 10, Data seen: 12000] loss: 0.895\n",
      "[Epoch: 10, Data seen: 13000] loss: 0.907\n",
      "[Epoch: 10, Data seen: 14000] loss: 0.882\n",
      "[Epoch: 10, Data seen: 15000] loss: 0.894\n",
      "[Epoch: 10, Data seen: 16000] loss: 0.901\n",
      "[Epoch: 10, Data seen: 17000] loss: 0.954\n",
      "[Epoch: 10, Data seen: 18000] loss: 0.927\n",
      "[Epoch: 10, Data seen: 19000] loss: 0.929\n",
      "[Epoch: 10, Data seen: 20000] loss: 0.906\n",
      "[Epoch: 10, Data seen: 21000] loss: 0.887\n",
      "[Epoch: 10, Data seen: 22000] loss: 0.831\n",
      "[Epoch: 10, Data seen: 23000] loss: 0.895\n",
      "[Epoch: 10, Data seen: 24000] loss: 0.875\n",
      "[Epoch: 10, Data seen: 25000] loss: 0.903\n",
      "[Epoch: 10, Data seen: 26000] loss: 0.944\n",
      "[Epoch: 10, Data seen: 27000] loss: 0.902\n",
      "[Epoch: 10, Data seen: 28000] loss: 0.896\n",
      "[Epoch: 10, Data seen: 29000] loss: 0.929\n",
      "[Epoch: 10, Data seen: 30000] loss: 0.878\n",
      "[Epoch: 10, Data seen: 31000] loss: 0.916\n",
      "[Epoch: 10, Data seen: 32000] loss: 0.891\n",
      "[Epoch: 10, Data seen: 33000] loss: 0.889\n",
      "[Epoch: 10, Data seen: 34000] loss: 0.861\n",
      "[Epoch: 10, Data seen: 35000] loss: 0.903\n",
      "[Epoch: 10, Data seen: 36000] loss: 0.931\n",
      "[Epoch: 10, Data seen: 37000] loss: 0.910\n",
      "[Epoch: 10, Data seen: 38000] loss: 0.927\n",
      "[Epoch: 10, Data seen: 39000] loss: 0.915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 10, Data seen: 40000] loss: 0.896\n",
      "[Epoch: 11, Data seen:  1000] loss: 0.919\n",
      "[Epoch: 11, Data seen:  2000] loss: 0.885\n",
      "[Epoch: 11, Data seen:  3000] loss: 0.921\n",
      "[Epoch: 11, Data seen:  4000] loss: 0.938\n",
      "[Epoch: 11, Data seen:  5000] loss: 0.849\n",
      "[Epoch: 11, Data seen:  6000] loss: 0.919\n",
      "[Epoch: 11, Data seen:  7000] loss: 0.897\n",
      "[Epoch: 11, Data seen:  8000] loss: 0.941\n",
      "[Epoch: 11, Data seen:  9000] loss: 0.913\n",
      "[Epoch: 11, Data seen: 10000] loss: 0.911\n",
      "[Epoch: 11, Data seen: 11000] loss: 0.868\n",
      "[Epoch: 11, Data seen: 12000] loss: 0.883\n",
      "[Epoch: 11, Data seen: 13000] loss: 0.896\n",
      "[Epoch: 11, Data seen: 14000] loss: 0.866\n",
      "[Epoch: 11, Data seen: 15000] loss: 0.881\n",
      "[Epoch: 11, Data seen: 16000] loss: 0.884\n",
      "[Epoch: 11, Data seen: 17000] loss: 0.933\n",
      "[Epoch: 11, Data seen: 18000] loss: 0.921\n",
      "[Epoch: 11, Data seen: 19000] loss: 0.909\n",
      "[Epoch: 11, Data seen: 20000] loss: 0.898\n",
      "[Epoch: 11, Data seen: 21000] loss: 0.881\n",
      "[Epoch: 11, Data seen: 22000] loss: 0.817\n",
      "[Epoch: 11, Data seen: 23000] loss: 0.882\n",
      "[Epoch: 11, Data seen: 24000] loss: 0.862\n",
      "[Epoch: 11, Data seen: 25000] loss: 0.890\n",
      "[Epoch: 11, Data seen: 26000] loss: 0.923\n",
      "[Epoch: 11, Data seen: 27000] loss: 0.881\n",
      "[Epoch: 11, Data seen: 28000] loss: 0.882\n",
      "[Epoch: 11, Data seen: 29000] loss: 0.929\n",
      "[Epoch: 11, Data seen: 30000] loss: 0.866\n",
      "[Epoch: 11, Data seen: 31000] loss: 0.894\n",
      "[Epoch: 11, Data seen: 32000] loss: 0.872\n",
      "[Epoch: 11, Data seen: 33000] loss: 0.878\n",
      "[Epoch: 11, Data seen: 34000] loss: 0.845\n",
      "[Epoch: 11, Data seen: 35000] loss: 0.885\n",
      "[Epoch: 11, Data seen: 36000] loss: 0.910\n",
      "[Epoch: 11, Data seen: 37000] loss: 0.891\n",
      "[Epoch: 11, Data seen: 38000] loss: 0.903\n",
      "[Epoch: 11, Data seen: 39000] loss: 0.906\n",
      "[Epoch: 11, Data seen: 40000] loss: 0.880\n",
      "[Epoch: 12, Data seen:  1000] loss: 0.908\n",
      "[Epoch: 12, Data seen:  2000] loss: 0.879\n",
      "[Epoch: 12, Data seen:  3000] loss: 0.909\n",
      "[Epoch: 12, Data seen:  4000] loss: 0.927\n",
      "[Epoch: 12, Data seen:  5000] loss: 0.842\n",
      "[Epoch: 12, Data seen:  6000] loss: 0.910\n",
      "[Epoch: 12, Data seen:  7000] loss: 0.883\n",
      "[Epoch: 12, Data seen:  8000] loss: 0.939\n",
      "[Epoch: 12, Data seen:  9000] loss: 0.896\n",
      "[Epoch: 12, Data seen: 10000] loss: 0.893\n",
      "[Epoch: 12, Data seen: 11000] loss: 0.856\n",
      "[Epoch: 12, Data seen: 12000] loss: 0.867\n",
      "[Epoch: 12, Data seen: 13000] loss: 0.883\n",
      "[Epoch: 12, Data seen: 14000] loss: 0.860\n",
      "[Epoch: 12, Data seen: 15000] loss: 0.870\n",
      "[Epoch: 12, Data seen: 16000] loss: 0.879\n",
      "[Epoch: 12, Data seen: 17000] loss: 0.920\n",
      "[Epoch: 12, Data seen: 18000] loss: 0.918\n",
      "[Epoch: 12, Data seen: 19000] loss: 0.893\n",
      "[Epoch: 12, Data seen: 20000] loss: 0.883\n",
      "[Epoch: 12, Data seen: 21000] loss: 0.877\n",
      "[Epoch: 12, Data seen: 22000] loss: 0.801\n",
      "[Epoch: 12, Data seen: 23000] loss: 0.859\n",
      "[Epoch: 12, Data seen: 24000] loss: 0.853\n",
      "[Epoch: 12, Data seen: 25000] loss: 0.880\n",
      "[Epoch: 12, Data seen: 26000] loss: 0.910\n",
      "[Epoch: 12, Data seen: 27000] loss: 0.867\n",
      "[Epoch: 12, Data seen: 28000] loss: 0.874\n",
      "[Epoch: 12, Data seen: 29000] loss: 0.919\n",
      "[Epoch: 12, Data seen: 30000] loss: 0.856\n",
      "[Epoch: 12, Data seen: 31000] loss: 0.883\n",
      "[Epoch: 12, Data seen: 32000] loss: 0.869\n",
      "[Epoch: 12, Data seen: 33000] loss: 0.863\n",
      "[Epoch: 12, Data seen: 34000] loss: 0.830\n",
      "[Epoch: 12, Data seen: 35000] loss: 0.870\n",
      "[Epoch: 12, Data seen: 36000] loss: 0.900\n",
      "[Epoch: 12, Data seen: 37000] loss: 0.881\n",
      "[Epoch: 12, Data seen: 38000] loss: 0.881\n",
      "[Epoch: 12, Data seen: 39000] loss: 0.893\n",
      "[Epoch: 12, Data seen: 40000] loss: 0.874\n",
      "[Epoch: 13, Data seen:  1000] loss: 0.900\n",
      "[Epoch: 13, Data seen:  2000] loss: 0.874\n",
      "[Epoch: 13, Data seen:  3000] loss: 0.891\n",
      "[Epoch: 13, Data seen:  4000] loss: 0.915\n",
      "[Epoch: 13, Data seen:  5000] loss: 0.831\n",
      "[Epoch: 13, Data seen:  6000] loss: 0.908\n",
      "[Epoch: 13, Data seen:  7000] loss: 0.880\n",
      "[Epoch: 13, Data seen:  8000] loss: 0.926\n",
      "[Epoch: 13, Data seen:  9000] loss: 0.887\n",
      "[Epoch: 13, Data seen: 10000] loss: 0.880\n",
      "[Epoch: 13, Data seen: 11000] loss: 0.844\n",
      "[Epoch: 13, Data seen: 12000] loss: 0.855\n",
      "[Epoch: 13, Data seen: 13000] loss: 0.874\n",
      "[Epoch: 13, Data seen: 14000] loss: 0.855\n",
      "[Epoch: 13, Data seen: 15000] loss: 0.864\n",
      "[Epoch: 13, Data seen: 16000] loss: 0.866\n",
      "[Epoch: 13, Data seen: 17000] loss: 0.911\n",
      "[Epoch: 13, Data seen: 18000] loss: 0.899\n",
      "[Epoch: 13, Data seen: 19000] loss: 0.893\n",
      "[Epoch: 13, Data seen: 20000] loss: 0.871\n",
      "[Epoch: 13, Data seen: 21000] loss: 0.864\n",
      "[Epoch: 13, Data seen: 22000] loss: 0.795\n",
      "[Epoch: 13, Data seen: 23000] loss: 0.846\n",
      "[Epoch: 13, Data seen: 24000] loss: 0.843\n",
      "[Epoch: 13, Data seen: 25000] loss: 0.871\n",
      "[Epoch: 13, Data seen: 26000] loss: 0.898\n",
      "[Epoch: 13, Data seen: 27000] loss: 0.862\n",
      "[Epoch: 13, Data seen: 28000] loss: 0.865\n",
      "[Epoch: 13, Data seen: 29000] loss: 0.908\n",
      "[Epoch: 13, Data seen: 30000] loss: 0.850\n",
      "[Epoch: 13, Data seen: 31000] loss: 0.866\n",
      "[Epoch: 13, Data seen: 32000] loss: 0.855\n",
      "[Epoch: 13, Data seen: 33000] loss: 0.855\n",
      "[Epoch: 13, Data seen: 34000] loss: 0.819\n",
      "[Epoch: 13, Data seen: 35000] loss: 0.854\n",
      "[Epoch: 13, Data seen: 36000] loss: 0.885\n",
      "[Epoch: 13, Data seen: 37000] loss: 0.867\n",
      "[Epoch: 13, Data seen: 38000] loss: 0.874\n",
      "[Epoch: 13, Data seen: 39000] loss: 0.880\n",
      "[Epoch: 13, Data seen: 40000] loss: 0.863\n",
      "[Epoch: 14, Data seen:  1000] loss: 0.883\n",
      "[Epoch: 14, Data seen:  2000] loss: 0.866\n",
      "[Epoch: 14, Data seen:  3000] loss: 0.886\n",
      "[Epoch: 14, Data seen:  4000] loss: 0.905\n",
      "[Epoch: 14, Data seen:  5000] loss: 0.821\n",
      "[Epoch: 14, Data seen:  6000] loss: 0.894\n",
      "[Epoch: 14, Data seen:  7000] loss: 0.871\n",
      "[Epoch: 14, Data seen:  8000] loss: 0.915\n",
      "[Epoch: 14, Data seen:  9000] loss: 0.879\n",
      "[Epoch: 14, Data seen: 10000] loss: 0.870\n",
      "[Epoch: 14, Data seen: 11000] loss: 0.835\n",
      "[Epoch: 14, Data seen: 12000] loss: 0.848\n",
      "[Epoch: 14, Data seen: 13000] loss: 0.867\n",
      "[Epoch: 14, Data seen: 14000] loss: 0.848\n",
      "[Epoch: 14, Data seen: 15000] loss: 0.856\n",
      "[Epoch: 14, Data seen: 16000] loss: 0.856\n",
      "[Epoch: 14, Data seen: 17000] loss: 0.898\n",
      "[Epoch: 14, Data seen: 18000] loss: 0.891\n",
      "[Epoch: 14, Data seen: 19000] loss: 0.883\n",
      "[Epoch: 14, Data seen: 20000] loss: 0.859\n",
      "[Epoch: 14, Data seen: 21000] loss: 0.854\n",
      "[Epoch: 14, Data seen: 22000] loss: 0.787\n",
      "[Epoch: 14, Data seen: 23000] loss: 0.841\n",
      "[Epoch: 14, Data seen: 24000] loss: 0.834\n",
      "[Epoch: 14, Data seen: 25000] loss: 0.859\n",
      "[Epoch: 14, Data seen: 26000] loss: 0.887\n",
      "[Epoch: 14, Data seen: 27000] loss: 0.851\n",
      "[Epoch: 14, Data seen: 28000] loss: 0.857\n",
      "[Epoch: 14, Data seen: 29000] loss: 0.897\n",
      "[Epoch: 14, Data seen: 30000] loss: 0.843\n",
      "[Epoch: 14, Data seen: 31000] loss: 0.858\n",
      "[Epoch: 14, Data seen: 32000] loss: 0.846\n",
      "[Epoch: 14, Data seen: 33000] loss: 0.846\n",
      "[Epoch: 14, Data seen: 34000] loss: 0.811\n",
      "[Epoch: 14, Data seen: 35000] loss: 0.849\n",
      "[Epoch: 14, Data seen: 36000] loss: 0.872\n",
      "[Epoch: 14, Data seen: 37000] loss: 0.855\n",
      "[Epoch: 14, Data seen: 38000] loss: 0.867\n",
      "[Epoch: 14, Data seen: 39000] loss: 0.871\n",
      "[Epoch: 14, Data seen: 40000] loss: 0.858\n",
      "[Epoch: 15, Data seen:  1000] loss: 0.873\n",
      "[Epoch: 15, Data seen:  2000] loss: 0.857\n",
      "[Epoch: 15, Data seen:  3000] loss: 0.878\n",
      "[Epoch: 15, Data seen:  4000] loss: 0.897\n",
      "[Epoch: 15, Data seen:  5000] loss: 0.808\n",
      "[Epoch: 15, Data seen:  6000] loss: 0.885\n",
      "[Epoch: 15, Data seen:  7000] loss: 0.861\n",
      "[Epoch: 15, Data seen:  8000] loss: 0.912\n",
      "[Epoch: 15, Data seen:  9000] loss: 0.872\n",
      "[Epoch: 15, Data seen: 10000] loss: 0.860\n",
      "[Epoch: 15, Data seen: 11000] loss: 0.824\n",
      "[Epoch: 15, Data seen: 12000] loss: 0.840\n",
      "[Epoch: 15, Data seen: 13000] loss: 0.858\n",
      "[Epoch: 15, Data seen: 14000] loss: 0.837\n",
      "[Epoch: 15, Data seen: 15000] loss: 0.848\n",
      "[Epoch: 15, Data seen: 16000] loss: 0.850\n",
      "[Epoch: 15, Data seen: 17000] loss: 0.884\n",
      "[Epoch: 15, Data seen: 18000] loss: 0.883\n",
      "[Epoch: 15, Data seen: 19000] loss: 0.879\n",
      "[Epoch: 15, Data seen: 20000] loss: 0.851\n",
      "[Epoch: 15, Data seen: 21000] loss: 0.844\n",
      "[Epoch: 15, Data seen: 22000] loss: 0.781\n",
      "[Epoch: 15, Data seen: 23000] loss: 0.829\n",
      "[Epoch: 15, Data seen: 24000] loss: 0.827\n",
      "[Epoch: 15, Data seen: 25000] loss: 0.857\n",
      "[Epoch: 15, Data seen: 26000] loss: 0.872\n",
      "[Epoch: 15, Data seen: 27000] loss: 0.845\n",
      "[Epoch: 15, Data seen: 28000] loss: 0.850\n",
      "[Epoch: 15, Data seen: 29000] loss: 0.889\n",
      "[Epoch: 15, Data seen: 30000] loss: 0.833\n",
      "[Epoch: 15, Data seen: 31000] loss: 0.853\n",
      "[Epoch: 15, Data seen: 32000] loss: 0.843\n",
      "[Epoch: 15, Data seen: 33000] loss: 0.837\n",
      "[Epoch: 15, Data seen: 34000] loss: 0.799\n",
      "[Epoch: 15, Data seen: 35000] loss: 0.838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 15, Data seen: 36000] loss: 0.862\n",
      "[Epoch: 15, Data seen: 37000] loss: 0.845\n",
      "[Epoch: 15, Data seen: 38000] loss: 0.861\n",
      "[Epoch: 15, Data seen: 39000] loss: 0.862\n",
      "[Epoch: 15, Data seen: 40000] loss: 0.849\n",
      "[Epoch: 16, Data seen:  1000] loss: 0.858\n",
      "[Epoch: 16, Data seen:  2000] loss: 0.854\n",
      "[Epoch: 16, Data seen:  3000] loss: 0.868\n",
      "[Epoch: 16, Data seen:  4000] loss: 0.890\n",
      "[Epoch: 16, Data seen:  5000] loss: 0.796\n",
      "[Epoch: 16, Data seen:  6000] loss: 0.877\n",
      "[Epoch: 16, Data seen:  7000] loss: 0.854\n",
      "[Epoch: 16, Data seen:  8000] loss: 0.908\n",
      "[Epoch: 16, Data seen:  9000] loss: 0.866\n",
      "[Epoch: 16, Data seen: 10000] loss: 0.855\n",
      "[Epoch: 16, Data seen: 11000] loss: 0.817\n",
      "[Epoch: 16, Data seen: 12000] loss: 0.830\n",
      "[Epoch: 16, Data seen: 13000] loss: 0.850\n",
      "[Epoch: 16, Data seen: 14000] loss: 0.832\n",
      "[Epoch: 16, Data seen: 15000] loss: 0.840\n",
      "[Epoch: 16, Data seen: 16000] loss: 0.839\n",
      "[Epoch: 16, Data seen: 17000] loss: 0.876\n",
      "[Epoch: 16, Data seen: 18000] loss: 0.874\n",
      "[Epoch: 16, Data seen: 19000] loss: 0.875\n",
      "[Epoch: 16, Data seen: 20000] loss: 0.843\n",
      "[Epoch: 16, Data seen: 21000] loss: 0.835\n",
      "[Epoch: 16, Data seen: 22000] loss: 0.773\n",
      "[Epoch: 16, Data seen: 23000] loss: 0.821\n",
      "[Epoch: 16, Data seen: 24000] loss: 0.821\n",
      "[Epoch: 16, Data seen: 25000] loss: 0.852\n",
      "[Epoch: 16, Data seen: 26000] loss: 0.865\n",
      "[Epoch: 16, Data seen: 27000] loss: 0.835\n",
      "[Epoch: 16, Data seen: 28000] loss: 0.837\n",
      "[Epoch: 16, Data seen: 29000] loss: 0.878\n",
      "[Epoch: 16, Data seen: 30000] loss: 0.827\n",
      "[Epoch: 16, Data seen: 31000] loss: 0.843\n",
      "[Epoch: 16, Data seen: 32000] loss: 0.834\n",
      "[Epoch: 16, Data seen: 33000] loss: 0.827\n",
      "[Epoch: 16, Data seen: 34000] loss: 0.790\n",
      "[Epoch: 16, Data seen: 35000] loss: 0.828\n",
      "[Epoch: 16, Data seen: 36000] loss: 0.856\n",
      "[Epoch: 16, Data seen: 37000] loss: 0.835\n",
      "[Epoch: 16, Data seen: 38000] loss: 0.856\n",
      "[Epoch: 16, Data seen: 39000] loss: 0.846\n",
      "[Epoch: 16, Data seen: 40000] loss: 0.842\n",
      "[Epoch: 17, Data seen:  1000] loss: 0.851\n",
      "[Epoch: 17, Data seen:  2000] loss: 0.843\n",
      "[Epoch: 17, Data seen:  3000] loss: 0.863\n",
      "[Epoch: 17, Data seen:  4000] loss: 0.883\n",
      "[Epoch: 17, Data seen:  5000] loss: 0.793\n",
      "[Epoch: 17, Data seen:  6000] loss: 0.873\n",
      "[Epoch: 17, Data seen:  7000] loss: 0.843\n",
      "[Epoch: 17, Data seen:  8000] loss: 0.904\n",
      "[Epoch: 17, Data seen:  9000] loss: 0.863\n",
      "[Epoch: 17, Data seen: 10000] loss: 0.845\n",
      "[Epoch: 17, Data seen: 11000] loss: 0.810\n",
      "[Epoch: 17, Data seen: 12000] loss: 0.822\n",
      "[Epoch: 17, Data seen: 13000] loss: 0.838\n",
      "[Epoch: 17, Data seen: 14000] loss: 0.825\n",
      "[Epoch: 17, Data seen: 15000] loss: 0.834\n",
      "[Epoch: 17, Data seen: 16000] loss: 0.834\n",
      "[Epoch: 17, Data seen: 17000] loss: 0.865\n",
      "[Epoch: 17, Data seen: 18000] loss: 0.863\n",
      "[Epoch: 17, Data seen: 19000] loss: 0.867\n",
      "[Epoch: 17, Data seen: 20000] loss: 0.840\n",
      "[Epoch: 17, Data seen: 21000] loss: 0.828\n",
      "[Epoch: 17, Data seen: 22000] loss: 0.767\n",
      "[Epoch: 17, Data seen: 23000] loss: 0.811\n",
      "[Epoch: 17, Data seen: 24000] loss: 0.810\n",
      "[Epoch: 17, Data seen: 25000] loss: 0.841\n",
      "[Epoch: 17, Data seen: 26000] loss: 0.856\n",
      "[Epoch: 17, Data seen: 27000] loss: 0.830\n",
      "[Epoch: 17, Data seen: 28000] loss: 0.832\n",
      "[Epoch: 17, Data seen: 29000] loss: 0.868\n",
      "[Epoch: 17, Data seen: 30000] loss: 0.825\n",
      "[Epoch: 17, Data seen: 31000] loss: 0.836\n",
      "[Epoch: 17, Data seen: 32000] loss: 0.826\n",
      "[Epoch: 17, Data seen: 33000] loss: 0.819\n",
      "[Epoch: 17, Data seen: 34000] loss: 0.777\n",
      "[Epoch: 17, Data seen: 35000] loss: 0.823\n",
      "[Epoch: 17, Data seen: 36000] loss: 0.852\n",
      "[Epoch: 17, Data seen: 37000] loss: 0.828\n",
      "[Epoch: 17, Data seen: 38000] loss: 0.847\n",
      "[Epoch: 17, Data seen: 39000] loss: 0.837\n",
      "[Epoch: 17, Data seen: 40000] loss: 0.835\n",
      "[Epoch: 18, Data seen:  1000] loss: 0.846\n",
      "[Epoch: 18, Data seen:  2000] loss: 0.836\n",
      "[Epoch: 18, Data seen:  3000] loss: 0.855\n",
      "[Epoch: 18, Data seen:  4000] loss: 0.880\n",
      "[Epoch: 18, Data seen:  5000] loss: 0.788\n",
      "[Epoch: 18, Data seen:  6000] loss: 0.870\n",
      "[Epoch: 18, Data seen:  7000] loss: 0.833\n",
      "[Epoch: 18, Data seen:  8000] loss: 0.895\n",
      "[Epoch: 18, Data seen:  9000] loss: 0.855\n",
      "[Epoch: 18, Data seen: 10000] loss: 0.836\n",
      "[Epoch: 18, Data seen: 11000] loss: 0.805\n",
      "[Epoch: 18, Data seen: 12000] loss: 0.819\n",
      "[Epoch: 18, Data seen: 13000] loss: 0.831\n",
      "[Epoch: 18, Data seen: 14000] loss: 0.819\n",
      "[Epoch: 18, Data seen: 15000] loss: 0.829\n",
      "[Epoch: 18, Data seen: 16000] loss: 0.829\n",
      "[Epoch: 18, Data seen: 17000] loss: 0.856\n",
      "[Epoch: 18, Data seen: 18000] loss: 0.855\n",
      "[Epoch: 18, Data seen: 19000] loss: 0.865\n",
      "[Epoch: 18, Data seen: 20000] loss: 0.832\n",
      "[Epoch: 18, Data seen: 21000] loss: 0.822\n",
      "[Epoch: 18, Data seen: 22000] loss: 0.759\n",
      "[Epoch: 18, Data seen: 23000] loss: 0.805\n",
      "[Epoch: 18, Data seen: 24000] loss: 0.803\n",
      "[Epoch: 18, Data seen: 25000] loss: 0.837\n",
      "[Epoch: 18, Data seen: 26000] loss: 0.847\n",
      "[Epoch: 18, Data seen: 27000] loss: 0.828\n",
      "[Epoch: 18, Data seen: 28000] loss: 0.829\n",
      "[Epoch: 18, Data seen: 29000] loss: 0.862\n",
      "[Epoch: 18, Data seen: 30000] loss: 0.818\n",
      "[Epoch: 18, Data seen: 31000] loss: 0.830\n",
      "[Epoch: 18, Data seen: 32000] loss: 0.821\n",
      "[Epoch: 18, Data seen: 33000] loss: 0.813\n",
      "[Epoch: 18, Data seen: 34000] loss: 0.773\n",
      "[Epoch: 18, Data seen: 35000] loss: 0.818\n",
      "[Epoch: 18, Data seen: 36000] loss: 0.842\n",
      "[Epoch: 18, Data seen: 37000] loss: 0.820\n",
      "[Epoch: 18, Data seen: 38000] loss: 0.839\n",
      "[Epoch: 18, Data seen: 39000] loss: 0.831\n",
      "[Epoch: 18, Data seen: 40000] loss: 0.828\n",
      "[Epoch: 19, Data seen:  1000] loss: 0.841\n",
      "[Epoch: 19, Data seen:  2000] loss: 0.831\n",
      "[Epoch: 19, Data seen:  3000] loss: 0.852\n",
      "[Epoch: 19, Data seen:  4000] loss: 0.877\n",
      "[Epoch: 19, Data seen:  5000] loss: 0.783\n",
      "[Epoch: 19, Data seen:  6000] loss: 0.863\n",
      "[Epoch: 19, Data seen:  7000] loss: 0.825\n",
      "[Epoch: 19, Data seen:  8000] loss: 0.887\n",
      "[Epoch: 19, Data seen:  9000] loss: 0.852\n",
      "[Epoch: 19, Data seen: 10000] loss: 0.829\n",
      "[Epoch: 19, Data seen: 11000] loss: 0.801\n",
      "[Epoch: 19, Data seen: 12000] loss: 0.812\n",
      "[Epoch: 19, Data seen: 13000] loss: 0.830\n",
      "[Epoch: 19, Data seen: 14000] loss: 0.813\n",
      "[Epoch: 19, Data seen: 15000] loss: 0.821\n",
      "[Epoch: 19, Data seen: 16000] loss: 0.825\n",
      "[Epoch: 19, Data seen: 17000] loss: 0.852\n",
      "[Epoch: 19, Data seen: 18000] loss: 0.852\n",
      "[Epoch: 19, Data seen: 19000] loss: 0.858\n",
      "[Epoch: 19, Data seen: 20000] loss: 0.828\n",
      "[Epoch: 19, Data seen: 21000] loss: 0.815\n",
      "[Epoch: 19, Data seen: 22000] loss: 0.753\n",
      "[Epoch: 19, Data seen: 23000] loss: 0.802\n",
      "[Epoch: 19, Data seen: 24000] loss: 0.798\n",
      "[Epoch: 19, Data seen: 25000] loss: 0.833\n",
      "[Epoch: 19, Data seen: 26000] loss: 0.842\n",
      "[Epoch: 19, Data seen: 27000] loss: 0.822\n",
      "[Epoch: 19, Data seen: 28000] loss: 0.822\n",
      "[Epoch: 19, Data seen: 29000] loss: 0.856\n",
      "[Epoch: 19, Data seen: 30000] loss: 0.815\n",
      "[Epoch: 19, Data seen: 31000] loss: 0.824\n",
      "[Epoch: 19, Data seen: 32000] loss: 0.818\n",
      "[Epoch: 19, Data seen: 33000] loss: 0.806\n",
      "[Epoch: 19, Data seen: 34000] loss: 0.762\n",
      "[Epoch: 19, Data seen: 35000] loss: 0.813\n",
      "[Epoch: 19, Data seen: 36000] loss: 0.836\n",
      "[Epoch: 19, Data seen: 37000] loss: 0.813\n",
      "[Epoch: 19, Data seen: 38000] loss: 0.832\n",
      "[Epoch: 19, Data seen: 39000] loss: 0.823\n",
      "[Epoch: 19, Data seen: 40000] loss: 0.821\n",
      "[Epoch: 20, Data seen:  1000] loss: 0.837\n",
      "[Epoch: 20, Data seen:  2000] loss: 0.826\n",
      "[Epoch: 20, Data seen:  3000] loss: 0.847\n",
      "[Epoch: 20, Data seen:  4000] loss: 0.872\n",
      "[Epoch: 20, Data seen:  5000] loss: 0.778\n",
      "[Epoch: 20, Data seen:  6000] loss: 0.856\n",
      "[Epoch: 20, Data seen:  7000] loss: 0.817\n",
      "[Epoch: 20, Data seen:  8000] loss: 0.884\n",
      "[Epoch: 20, Data seen:  9000] loss: 0.848\n",
      "[Epoch: 20, Data seen: 10000] loss: 0.822\n",
      "[Epoch: 20, Data seen: 11000] loss: 0.798\n",
      "[Epoch: 20, Data seen: 12000] loss: 0.808\n",
      "[Epoch: 20, Data seen: 13000] loss: 0.823\n",
      "[Epoch: 20, Data seen: 14000] loss: 0.807\n",
      "[Epoch: 20, Data seen: 15000] loss: 0.813\n",
      "[Epoch: 20, Data seen: 16000] loss: 0.816\n",
      "[Epoch: 20, Data seen: 17000] loss: 0.845\n",
      "[Epoch: 20, Data seen: 18000] loss: 0.844\n",
      "[Epoch: 20, Data seen: 19000] loss: 0.858\n",
      "[Epoch: 20, Data seen: 20000] loss: 0.825\n",
      "[Epoch: 20, Data seen: 21000] loss: 0.814\n",
      "[Epoch: 20, Data seen: 22000] loss: 0.750\n",
      "[Epoch: 20, Data seen: 23000] loss: 0.797\n",
      "[Epoch: 20, Data seen: 24000] loss: 0.795\n",
      "[Epoch: 20, Data seen: 25000] loss: 0.828\n",
      "[Epoch: 20, Data seen: 26000] loss: 0.838\n",
      "[Epoch: 20, Data seen: 27000] loss: 0.817\n",
      "[Epoch: 20, Data seen: 28000] loss: 0.815\n",
      "[Epoch: 20, Data seen: 29000] loss: 0.852\n",
      "[Epoch: 20, Data seen: 30000] loss: 0.811\n",
      "[Epoch: 20, Data seen: 31000] loss: 0.821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 20, Data seen: 32000] loss: 0.814\n",
      "[Epoch: 20, Data seen: 33000] loss: 0.797\n",
      "[Epoch: 20, Data seen: 34000] loss: 0.757\n",
      "[Epoch: 20, Data seen: 35000] loss: 0.810\n",
      "[Epoch: 20, Data seen: 36000] loss: 0.831\n",
      "[Epoch: 20, Data seen: 37000] loss: 0.808\n",
      "[Epoch: 20, Data seen: 38000] loss: 0.830\n",
      "[Epoch: 20, Data seen: 39000] loss: 0.813\n",
      "[Epoch: 20, Data seen: 40000] loss: 0.813\n",
      "[Epoch: 21, Data seen:  1000] loss: 0.832\n",
      "[Epoch: 21, Data seen:  2000] loss: 0.822\n",
      "[Epoch: 21, Data seen:  3000] loss: 0.844\n",
      "[Epoch: 21, Data seen:  4000] loss: 0.869\n",
      "[Epoch: 21, Data seen:  5000] loss: 0.773\n",
      "[Epoch: 21, Data seen:  6000] loss: 0.854\n",
      "[Epoch: 21, Data seen:  7000] loss: 0.810\n",
      "[Epoch: 21, Data seen:  8000] loss: 0.877\n",
      "[Epoch: 21, Data seen:  9000] loss: 0.845\n",
      "[Epoch: 21, Data seen: 10000] loss: 0.820\n",
      "[Epoch: 21, Data seen: 11000] loss: 0.790\n",
      "[Epoch: 21, Data seen: 12000] loss: 0.801\n",
      "[Epoch: 21, Data seen: 13000] loss: 0.818\n",
      "[Epoch: 21, Data seen: 14000] loss: 0.803\n",
      "[Epoch: 21, Data seen: 15000] loss: 0.807\n",
      "[Epoch: 21, Data seen: 16000] loss: 0.812\n",
      "[Epoch: 21, Data seen: 17000] loss: 0.839\n",
      "[Epoch: 21, Data seen: 18000] loss: 0.842\n",
      "[Epoch: 21, Data seen: 19000] loss: 0.851\n",
      "[Epoch: 21, Data seen: 20000] loss: 0.821\n",
      "[Epoch: 21, Data seen: 21000] loss: 0.808\n",
      "[Epoch: 21, Data seen: 22000] loss: 0.747\n",
      "[Epoch: 21, Data seen: 23000] loss: 0.797\n",
      "[Epoch: 21, Data seen: 24000] loss: 0.792\n",
      "[Epoch: 21, Data seen: 25000] loss: 0.825\n",
      "[Epoch: 21, Data seen: 26000] loss: 0.833\n",
      "[Epoch: 21, Data seen: 27000] loss: 0.815\n",
      "[Epoch: 21, Data seen: 28000] loss: 0.811\n",
      "[Epoch: 21, Data seen: 29000] loss: 0.847\n",
      "[Epoch: 21, Data seen: 30000] loss: 0.808\n",
      "[Epoch: 21, Data seen: 31000] loss: 0.815\n",
      "[Epoch: 21, Data seen: 32000] loss: 0.810\n",
      "[Epoch: 21, Data seen: 33000] loss: 0.791\n",
      "[Epoch: 21, Data seen: 34000] loss: 0.749\n",
      "[Epoch: 21, Data seen: 35000] loss: 0.805\n",
      "[Epoch: 21, Data seen: 36000] loss: 0.827\n",
      "[Epoch: 21, Data seen: 37000] loss: 0.804\n",
      "[Epoch: 21, Data seen: 38000] loss: 0.826\n",
      "[Epoch: 21, Data seen: 39000] loss: 0.809\n",
      "[Epoch: 21, Data seen: 40000] loss: 0.806\n",
      "[Epoch: 22, Data seen:  1000] loss: 0.828\n",
      "[Epoch: 22, Data seen:  2000] loss: 0.816\n",
      "[Epoch: 22, Data seen:  3000] loss: 0.837\n",
      "[Epoch: 22, Data seen:  4000] loss: 0.863\n",
      "[Epoch: 22, Data seen:  5000] loss: 0.769\n",
      "[Epoch: 22, Data seen:  6000] loss: 0.849\n",
      "[Epoch: 22, Data seen:  7000] loss: 0.802\n",
      "[Epoch: 22, Data seen:  8000] loss: 0.873\n",
      "[Epoch: 22, Data seen:  9000] loss: 0.840\n",
      "[Epoch: 22, Data seen: 10000] loss: 0.817\n",
      "[Epoch: 22, Data seen: 11000] loss: 0.787\n",
      "[Epoch: 22, Data seen: 12000] loss: 0.800\n",
      "[Epoch: 22, Data seen: 13000] loss: 0.815\n",
      "[Epoch: 22, Data seen: 14000] loss: 0.795\n",
      "[Epoch: 22, Data seen: 15000] loss: 0.803\n",
      "[Epoch: 22, Data seen: 16000] loss: 0.809\n",
      "[Epoch: 22, Data seen: 17000] loss: 0.834\n",
      "[Epoch: 22, Data seen: 18000] loss: 0.835\n",
      "[Epoch: 22, Data seen: 19000] loss: 0.848\n",
      "[Epoch: 22, Data seen: 20000] loss: 0.818\n",
      "[Epoch: 22, Data seen: 21000] loss: 0.803\n",
      "[Epoch: 22, Data seen: 22000] loss: 0.745\n",
      "[Epoch: 22, Data seen: 23000] loss: 0.793\n",
      "[Epoch: 22, Data seen: 24000] loss: 0.789\n",
      "[Epoch: 22, Data seen: 25000] loss: 0.822\n",
      "[Epoch: 22, Data seen: 26000] loss: 0.829\n",
      "[Epoch: 22, Data seen: 27000] loss: 0.812\n",
      "[Epoch: 22, Data seen: 28000] loss: 0.808\n",
      "[Epoch: 22, Data seen: 29000] loss: 0.841\n",
      "[Epoch: 22, Data seen: 30000] loss: 0.807\n",
      "[Epoch: 22, Data seen: 31000] loss: 0.814\n",
      "[Epoch: 22, Data seen: 32000] loss: 0.806\n",
      "[Epoch: 22, Data seen: 33000] loss: 0.786\n",
      "[Epoch: 22, Data seen: 34000] loss: 0.745\n",
      "[Epoch: 22, Data seen: 35000] loss: 0.802\n",
      "[Epoch: 22, Data seen: 36000] loss: 0.823\n",
      "[Epoch: 22, Data seen: 37000] loss: 0.800\n",
      "[Epoch: 22, Data seen: 38000] loss: 0.819\n",
      "[Epoch: 22, Data seen: 39000] loss: 0.803\n",
      "[Epoch: 22, Data seen: 40000] loss: 0.801\n",
      "[Epoch: 23, Data seen:  1000] loss: 0.825\n",
      "[Epoch: 23, Data seen:  2000] loss: 0.814\n",
      "[Epoch: 23, Data seen:  3000] loss: 0.833\n",
      "[Epoch: 23, Data seen:  4000] loss: 0.860\n",
      "[Epoch: 23, Data seen:  5000] loss: 0.764\n",
      "[Epoch: 23, Data seen:  6000] loss: 0.845\n",
      "[Epoch: 23, Data seen:  7000] loss: 0.796\n",
      "[Epoch: 23, Data seen:  8000] loss: 0.868\n",
      "[Epoch: 23, Data seen:  9000] loss: 0.836\n",
      "[Epoch: 23, Data seen: 10000] loss: 0.815\n",
      "[Epoch: 23, Data seen: 11000] loss: 0.784\n",
      "[Epoch: 23, Data seen: 12000] loss: 0.794\n",
      "[Epoch: 23, Data seen: 13000] loss: 0.812\n",
      "[Epoch: 23, Data seen: 14000] loss: 0.794\n",
      "[Epoch: 23, Data seen: 15000] loss: 0.797\n",
      "[Epoch: 23, Data seen: 16000] loss: 0.802\n",
      "[Epoch: 23, Data seen: 17000] loss: 0.829\n",
      "[Epoch: 23, Data seen: 18000] loss: 0.832\n",
      "[Epoch: 23, Data seen: 19000] loss: 0.844\n",
      "[Epoch: 23, Data seen: 20000] loss: 0.815\n",
      "[Epoch: 23, Data seen: 21000] loss: 0.799\n",
      "[Epoch: 23, Data seen: 22000] loss: 0.743\n",
      "[Epoch: 23, Data seen: 23000] loss: 0.791\n",
      "[Epoch: 23, Data seen: 24000] loss: 0.784\n",
      "[Epoch: 23, Data seen: 25000] loss: 0.818\n",
      "[Epoch: 23, Data seen: 26000] loss: 0.824\n",
      "[Epoch: 23, Data seen: 27000] loss: 0.807\n",
      "[Epoch: 23, Data seen: 28000] loss: 0.805\n",
      "[Epoch: 23, Data seen: 29000] loss: 0.837\n",
      "[Epoch: 23, Data seen: 30000] loss: 0.808\n",
      "[Epoch: 23, Data seen: 31000] loss: 0.809\n",
      "[Epoch: 23, Data seen: 32000] loss: 0.802\n",
      "[Epoch: 23, Data seen: 33000] loss: 0.784\n",
      "[Epoch: 23, Data seen: 34000] loss: 0.740\n",
      "[Epoch: 23, Data seen: 35000] loss: 0.799\n",
      "[Epoch: 23, Data seen: 36000] loss: 0.816\n",
      "[Epoch: 23, Data seen: 37000] loss: 0.798\n",
      "[Epoch: 23, Data seen: 38000] loss: 0.814\n",
      "[Epoch: 23, Data seen: 39000] loss: 0.797\n",
      "[Epoch: 23, Data seen: 40000] loss: 0.797\n",
      "[Epoch: 24, Data seen:  1000] loss: 0.820\n",
      "[Epoch: 24, Data seen:  2000] loss: 0.811\n",
      "[Epoch: 24, Data seen:  3000] loss: 0.828\n",
      "[Epoch: 24, Data seen:  4000] loss: 0.857\n",
      "[Epoch: 24, Data seen:  5000] loss: 0.762\n",
      "[Epoch: 24, Data seen:  6000] loss: 0.843\n",
      "[Epoch: 24, Data seen:  7000] loss: 0.790\n",
      "[Epoch: 24, Data seen:  8000] loss: 0.864\n",
      "[Epoch: 24, Data seen:  9000] loss: 0.834\n",
      "[Epoch: 24, Data seen: 10000] loss: 0.810\n",
      "[Epoch: 24, Data seen: 11000] loss: 0.779\n",
      "[Epoch: 24, Data seen: 12000] loss: 0.788\n",
      "[Epoch: 24, Data seen: 13000] loss: 0.811\n",
      "[Epoch: 24, Data seen: 14000] loss: 0.790\n",
      "[Epoch: 24, Data seen: 15000] loss: 0.794\n",
      "[Epoch: 24, Data seen: 16000] loss: 0.798\n",
      "[Epoch: 24, Data seen: 17000] loss: 0.826\n",
      "[Epoch: 24, Data seen: 18000] loss: 0.827\n",
      "[Epoch: 24, Data seen: 19000] loss: 0.841\n",
      "[Epoch: 24, Data seen: 20000] loss: 0.812\n",
      "[Epoch: 24, Data seen: 21000] loss: 0.795\n",
      "[Epoch: 24, Data seen: 22000] loss: 0.741\n",
      "[Epoch: 24, Data seen: 23000] loss: 0.788\n",
      "[Epoch: 24, Data seen: 24000] loss: 0.780\n",
      "[Epoch: 24, Data seen: 25000] loss: 0.814\n",
      "[Epoch: 24, Data seen: 26000] loss: 0.820\n",
      "[Epoch: 24, Data seen: 27000] loss: 0.804\n",
      "[Epoch: 24, Data seen: 28000] loss: 0.801\n",
      "[Epoch: 24, Data seen: 29000] loss: 0.833\n",
      "[Epoch: 24, Data seen: 30000] loss: 0.809\n",
      "[Epoch: 24, Data seen: 31000] loss: 0.803\n",
      "[Epoch: 24, Data seen: 32000] loss: 0.798\n",
      "[Epoch: 24, Data seen: 33000] loss: 0.779\n",
      "[Epoch: 24, Data seen: 34000] loss: 0.737\n",
      "[Epoch: 24, Data seen: 35000] loss: 0.795\n",
      "[Epoch: 24, Data seen: 36000] loss: 0.811\n",
      "[Epoch: 24, Data seen: 37000] loss: 0.794\n",
      "[Epoch: 24, Data seen: 38000] loss: 0.810\n",
      "[Epoch: 24, Data seen: 39000] loss: 0.792\n",
      "[Epoch: 24, Data seen: 40000] loss: 0.795\n",
      "[Epoch: 25, Data seen:  1000] loss: 0.816\n",
      "[Epoch: 25, Data seen:  2000] loss: 0.807\n",
      "[Epoch: 25, Data seen:  3000] loss: 0.825\n",
      "[Epoch: 25, Data seen:  4000] loss: 0.854\n",
      "[Epoch: 25, Data seen:  5000] loss: 0.759\n",
      "[Epoch: 25, Data seen:  6000] loss: 0.840\n",
      "[Epoch: 25, Data seen:  7000] loss: 0.785\n",
      "[Epoch: 25, Data seen:  8000] loss: 0.860\n",
      "[Epoch: 25, Data seen:  9000] loss: 0.831\n",
      "[Epoch: 25, Data seen: 10000] loss: 0.808\n",
      "[Epoch: 25, Data seen: 11000] loss: 0.776\n",
      "[Epoch: 25, Data seen: 12000] loss: 0.785\n",
      "[Epoch: 25, Data seen: 13000] loss: 0.808\n",
      "[Epoch: 25, Data seen: 14000] loss: 0.787\n",
      "[Epoch: 25, Data seen: 15000] loss: 0.789\n",
      "[Epoch: 25, Data seen: 16000] loss: 0.795\n",
      "[Epoch: 25, Data seen: 17000] loss: 0.824\n",
      "[Epoch: 25, Data seen: 18000] loss: 0.823\n",
      "[Epoch: 25, Data seen: 19000] loss: 0.838\n",
      "[Epoch: 25, Data seen: 20000] loss: 0.811\n",
      "[Epoch: 25, Data seen: 21000] loss: 0.790\n",
      "[Epoch: 25, Data seen: 22000] loss: 0.739\n",
      "[Epoch: 25, Data seen: 23000] loss: 0.785\n",
      "[Epoch: 25, Data seen: 24000] loss: 0.778\n",
      "[Epoch: 25, Data seen: 25000] loss: 0.811\n",
      "[Epoch: 25, Data seen: 26000] loss: 0.818\n",
      "[Epoch: 25, Data seen: 27000] loss: 0.800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 25, Data seen: 28000] loss: 0.796\n",
      "[Epoch: 25, Data seen: 29000] loss: 0.832\n",
      "[Epoch: 25, Data seen: 30000] loss: 0.807\n",
      "[Epoch: 25, Data seen: 31000] loss: 0.802\n",
      "[Epoch: 25, Data seen: 32000] loss: 0.795\n",
      "[Epoch: 25, Data seen: 33000] loss: 0.777\n",
      "[Epoch: 25, Data seen: 34000] loss: 0.733\n",
      "[Epoch: 25, Data seen: 35000] loss: 0.792\n",
      "[Epoch: 25, Data seen: 36000] loss: 0.807\n",
      "[Epoch: 25, Data seen: 37000] loss: 0.790\n",
      "[Epoch: 25, Data seen: 38000] loss: 0.807\n",
      "[Epoch: 25, Data seen: 39000] loss: 0.789\n",
      "[Epoch: 25, Data seen: 40000] loss: 0.791\n",
      "[Epoch: 26, Data seen:  1000] loss: 0.814\n",
      "[Epoch: 26, Data seen:  2000] loss: 0.804\n",
      "[Epoch: 26, Data seen:  3000] loss: 0.820\n",
      "[Epoch: 26, Data seen:  4000] loss: 0.852\n",
      "[Epoch: 26, Data seen:  5000] loss: 0.758\n",
      "[Epoch: 26, Data seen:  6000] loss: 0.838\n",
      "[Epoch: 26, Data seen:  7000] loss: 0.781\n",
      "[Epoch: 26, Data seen:  8000] loss: 0.856\n",
      "[Epoch: 26, Data seen:  9000] loss: 0.828\n",
      "[Epoch: 26, Data seen: 10000] loss: 0.807\n",
      "[Epoch: 26, Data seen: 11000] loss: 0.772\n",
      "[Epoch: 26, Data seen: 12000] loss: 0.782\n",
      "[Epoch: 26, Data seen: 13000] loss: 0.805\n",
      "[Epoch: 26, Data seen: 14000] loss: 0.785\n",
      "[Epoch: 26, Data seen: 15000] loss: 0.787\n",
      "[Epoch: 26, Data seen: 16000] loss: 0.793\n",
      "[Epoch: 26, Data seen: 17000] loss: 0.821\n",
      "[Epoch: 26, Data seen: 18000] loss: 0.820\n",
      "[Epoch: 26, Data seen: 19000] loss: 0.834\n",
      "[Epoch: 26, Data seen: 20000] loss: 0.809\n",
      "[Epoch: 26, Data seen: 21000] loss: 0.786\n",
      "[Epoch: 26, Data seen: 22000] loss: 0.738\n",
      "[Epoch: 26, Data seen: 23000] loss: 0.782\n",
      "[Epoch: 26, Data seen: 24000] loss: 0.777\n",
      "[Epoch: 26, Data seen: 25000] loss: 0.808\n",
      "[Epoch: 26, Data seen: 26000] loss: 0.815\n",
      "[Epoch: 26, Data seen: 27000] loss: 0.797\n",
      "[Epoch: 26, Data seen: 28000] loss: 0.792\n",
      "[Epoch: 26, Data seen: 29000] loss: 0.829\n",
      "[Epoch: 26, Data seen: 30000] loss: 0.807\n",
      "[Epoch: 26, Data seen: 31000] loss: 0.799\n",
      "[Epoch: 26, Data seen: 32000] loss: 0.793\n",
      "[Epoch: 26, Data seen: 33000] loss: 0.773\n",
      "[Epoch: 26, Data seen: 34000] loss: 0.732\n",
      "[Epoch: 26, Data seen: 35000] loss: 0.789\n",
      "[Epoch: 26, Data seen: 36000] loss: 0.805\n",
      "[Epoch: 26, Data seen: 37000] loss: 0.786\n",
      "[Epoch: 26, Data seen: 38000] loss: 0.804\n",
      "[Epoch: 26, Data seen: 39000] loss: 0.786\n",
      "[Epoch: 26, Data seen: 40000] loss: 0.789\n",
      "[Epoch: 27, Data seen:  1000] loss: 0.810\n",
      "[Epoch: 27, Data seen:  2000] loss: 0.801\n",
      "[Epoch: 27, Data seen:  3000] loss: 0.816\n",
      "[Epoch: 27, Data seen:  4000] loss: 0.848\n",
      "[Epoch: 27, Data seen:  5000] loss: 0.756\n",
      "[Epoch: 27, Data seen:  6000] loss: 0.835\n",
      "[Epoch: 27, Data seen:  7000] loss: 0.778\n",
      "[Epoch: 27, Data seen:  8000] loss: 0.853\n",
      "[Epoch: 27, Data seen:  9000] loss: 0.827\n",
      "[Epoch: 27, Data seen: 10000] loss: 0.806\n",
      "[Epoch: 27, Data seen: 11000] loss: 0.768\n",
      "[Epoch: 27, Data seen: 12000] loss: 0.780\n",
      "[Epoch: 27, Data seen: 13000] loss: 0.803\n",
      "[Epoch: 27, Data seen: 14000] loss: 0.784\n",
      "[Epoch: 27, Data seen: 15000] loss: 0.785\n",
      "[Epoch: 27, Data seen: 16000] loss: 0.793\n",
      "[Epoch: 27, Data seen: 17000] loss: 0.818\n",
      "[Epoch: 27, Data seen: 18000] loss: 0.815\n",
      "[Epoch: 27, Data seen: 19000] loss: 0.831\n",
      "[Epoch: 27, Data seen: 20000] loss: 0.804\n",
      "[Epoch: 27, Data seen: 21000] loss: 0.784\n",
      "[Epoch: 27, Data seen: 22000] loss: 0.736\n",
      "[Epoch: 27, Data seen: 23000] loss: 0.779\n",
      "[Epoch: 27, Data seen: 24000] loss: 0.775\n",
      "[Epoch: 27, Data seen: 25000] loss: 0.806\n",
      "[Epoch: 27, Data seen: 26000] loss: 0.813\n",
      "[Epoch: 27, Data seen: 27000] loss: 0.794\n",
      "[Epoch: 27, Data seen: 28000] loss: 0.789\n",
      "[Epoch: 27, Data seen: 29000] loss: 0.826\n",
      "[Epoch: 27, Data seen: 30000] loss: 0.807\n",
      "[Epoch: 27, Data seen: 31000] loss: 0.797\n",
      "[Epoch: 27, Data seen: 32000] loss: 0.791\n",
      "[Epoch: 27, Data seen: 33000] loss: 0.772\n",
      "[Epoch: 27, Data seen: 34000] loss: 0.730\n",
      "[Epoch: 27, Data seen: 35000] loss: 0.786\n",
      "[Epoch: 27, Data seen: 36000] loss: 0.802\n",
      "[Epoch: 27, Data seen: 37000] loss: 0.783\n",
      "[Epoch: 27, Data seen: 38000] loss: 0.802\n",
      "[Epoch: 27, Data seen: 39000] loss: 0.782\n",
      "[Epoch: 27, Data seen: 40000] loss: 0.785\n",
      "[Epoch: 28, Data seen:  1000] loss: 0.808\n",
      "[Epoch: 28, Data seen:  2000] loss: 0.798\n",
      "[Epoch: 28, Data seen:  3000] loss: 0.813\n",
      "[Epoch: 28, Data seen:  4000] loss: 0.845\n",
      "[Epoch: 28, Data seen:  5000] loss: 0.754\n",
      "[Epoch: 28, Data seen:  6000] loss: 0.833\n",
      "[Epoch: 28, Data seen:  7000] loss: 0.775\n",
      "[Epoch: 28, Data seen:  8000] loss: 0.850\n",
      "[Epoch: 28, Data seen:  9000] loss: 0.825\n",
      "[Epoch: 28, Data seen: 10000] loss: 0.803\n",
      "[Epoch: 28, Data seen: 11000] loss: 0.766\n",
      "[Epoch: 28, Data seen: 12000] loss: 0.776\n",
      "[Epoch: 28, Data seen: 13000] loss: 0.801\n",
      "[Epoch: 28, Data seen: 14000] loss: 0.783\n",
      "[Epoch: 28, Data seen: 15000] loss: 0.783\n",
      "[Epoch: 28, Data seen: 16000] loss: 0.790\n",
      "[Epoch: 28, Data seen: 17000] loss: 0.815\n",
      "[Epoch: 28, Data seen: 18000] loss: 0.812\n",
      "[Epoch: 28, Data seen: 19000] loss: 0.828\n",
      "[Epoch: 28, Data seen: 20000] loss: 0.802\n",
      "[Epoch: 28, Data seen: 21000] loss: 0.779\n",
      "[Epoch: 28, Data seen: 22000] loss: 0.734\n",
      "[Epoch: 28, Data seen: 23000] loss: 0.777\n",
      "[Epoch: 28, Data seen: 24000] loss: 0.772\n",
      "[Epoch: 28, Data seen: 25000] loss: 0.805\n",
      "[Epoch: 28, Data seen: 26000] loss: 0.810\n",
      "[Epoch: 28, Data seen: 27000] loss: 0.792\n",
      "[Epoch: 28, Data seen: 28000] loss: 0.787\n",
      "[Epoch: 28, Data seen: 29000] loss: 0.824\n",
      "[Epoch: 28, Data seen: 30000] loss: 0.806\n",
      "[Epoch: 28, Data seen: 31000] loss: 0.795\n",
      "[Epoch: 28, Data seen: 32000] loss: 0.790\n",
      "[Epoch: 28, Data seen: 33000] loss: 0.770\n",
      "[Epoch: 28, Data seen: 34000] loss: 0.730\n",
      "[Epoch: 28, Data seen: 35000] loss: 0.786\n",
      "[Epoch: 28, Data seen: 36000] loss: 0.799\n",
      "[Epoch: 28, Data seen: 37000] loss: 0.780\n",
      "[Epoch: 28, Data seen: 38000] loss: 0.800\n",
      "[Epoch: 28, Data seen: 39000] loss: 0.779\n",
      "[Epoch: 28, Data seen: 40000] loss: 0.782\n",
      "[Epoch: 29, Data seen:  1000] loss: 0.807\n",
      "[Epoch: 29, Data seen:  2000] loss: 0.794\n",
      "[Epoch: 29, Data seen:  3000] loss: 0.809\n",
      "[Epoch: 29, Data seen:  4000] loss: 0.845\n",
      "[Epoch: 29, Data seen:  5000] loss: 0.753\n",
      "[Epoch: 29, Data seen:  6000] loss: 0.831\n",
      "[Epoch: 29, Data seen:  7000] loss: 0.773\n",
      "[Epoch: 29, Data seen:  8000] loss: 0.847\n",
      "[Epoch: 29, Data seen:  9000] loss: 0.823\n",
      "[Epoch: 29, Data seen: 10000] loss: 0.802\n",
      "[Epoch: 29, Data seen: 11000] loss: 0.763\n",
      "[Epoch: 29, Data seen: 12000] loss: 0.775\n",
      "[Epoch: 29, Data seen: 13000] loss: 0.800\n",
      "[Epoch: 29, Data seen: 14000] loss: 0.782\n",
      "[Epoch: 29, Data seen: 15000] loss: 0.781\n",
      "[Epoch: 29, Data seen: 16000] loss: 0.788\n",
      "[Epoch: 29, Data seen: 17000] loss: 0.813\n",
      "[Epoch: 29, Data seen: 18000] loss: 0.809\n",
      "[Epoch: 29, Data seen: 19000] loss: 0.825\n",
      "[Epoch: 29, Data seen: 20000] loss: 0.800\n",
      "[Epoch: 29, Data seen: 21000] loss: 0.777\n",
      "[Epoch: 29, Data seen: 22000] loss: 0.732\n",
      "[Epoch: 29, Data seen: 23000] loss: 0.775\n",
      "[Epoch: 29, Data seen: 24000] loss: 0.770\n",
      "[Epoch: 29, Data seen: 25000] loss: 0.804\n",
      "[Epoch: 29, Data seen: 26000] loss: 0.808\n",
      "[Epoch: 29, Data seen: 27000] loss: 0.789\n",
      "[Epoch: 29, Data seen: 28000] loss: 0.784\n",
      "[Epoch: 29, Data seen: 29000] loss: 0.822\n",
      "[Epoch: 29, Data seen: 30000] loss: 0.805\n",
      "[Epoch: 29, Data seen: 31000] loss: 0.793\n",
      "[Epoch: 29, Data seen: 32000] loss: 0.790\n",
      "[Epoch: 29, Data seen: 33000] loss: 0.768\n",
      "[Epoch: 29, Data seen: 34000] loss: 0.730\n",
      "[Epoch: 29, Data seen: 35000] loss: 0.784\n",
      "[Epoch: 29, Data seen: 36000] loss: 0.797\n",
      "[Epoch: 29, Data seen: 37000] loss: 0.778\n",
      "[Epoch: 29, Data seen: 38000] loss: 0.797\n",
      "[Epoch: 29, Data seen: 39000] loss: 0.775\n",
      "[Epoch: 29, Data seen: 40000] loss: 0.779\n",
      "[Epoch: 30, Data seen:  1000] loss: 0.805\n",
      "[Epoch: 30, Data seen:  2000] loss: 0.792\n",
      "[Epoch: 30, Data seen:  3000] loss: 0.808\n",
      "[Epoch: 30, Data seen:  4000] loss: 0.843\n",
      "[Epoch: 30, Data seen:  5000] loss: 0.751\n",
      "[Epoch: 30, Data seen:  6000] loss: 0.830\n",
      "[Epoch: 30, Data seen:  7000] loss: 0.771\n",
      "[Epoch: 30, Data seen:  8000] loss: 0.842\n",
      "[Epoch: 30, Data seen:  9000] loss: 0.821\n",
      "[Epoch: 30, Data seen: 10000] loss: 0.802\n",
      "[Epoch: 30, Data seen: 11000] loss: 0.761\n",
      "[Epoch: 30, Data seen: 12000] loss: 0.774\n",
      "[Epoch: 30, Data seen: 13000] loss: 0.799\n",
      "[Epoch: 30, Data seen: 14000] loss: 0.779\n",
      "[Epoch: 30, Data seen: 15000] loss: 0.778\n",
      "[Epoch: 30, Data seen: 16000] loss: 0.786\n",
      "[Epoch: 30, Data seen: 17000] loss: 0.811\n",
      "[Epoch: 30, Data seen: 18000] loss: 0.808\n",
      "[Epoch: 30, Data seen: 19000] loss: 0.822\n",
      "[Epoch: 30, Data seen: 20000] loss: 0.797\n",
      "[Epoch: 30, Data seen: 21000] loss: 0.774\n",
      "[Epoch: 30, Data seen: 22000] loss: 0.730\n",
      "[Epoch: 30, Data seen: 23000] loss: 0.772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 30, Data seen: 24000] loss: 0.769\n",
      "[Epoch: 30, Data seen: 25000] loss: 0.802\n",
      "[Epoch: 30, Data seen: 26000] loss: 0.808\n",
      "[Epoch: 30, Data seen: 27000] loss: 0.787\n",
      "[Epoch: 30, Data seen: 28000] loss: 0.781\n",
      "[Epoch: 30, Data seen: 29000] loss: 0.820\n",
      "[Epoch: 30, Data seen: 30000] loss: 0.804\n",
      "[Epoch: 30, Data seen: 31000] loss: 0.791\n",
      "[Epoch: 30, Data seen: 32000] loss: 0.789\n",
      "[Epoch: 30, Data seen: 33000] loss: 0.766\n",
      "[Epoch: 30, Data seen: 34000] loss: 0.728\n",
      "[Epoch: 30, Data seen: 35000] loss: 0.783\n",
      "[Epoch: 30, Data seen: 36000] loss: 0.795\n",
      "[Epoch: 30, Data seen: 37000] loss: 0.777\n",
      "[Epoch: 30, Data seen: 38000] loss: 0.797\n",
      "[Epoch: 30, Data seen: 39000] loss: 0.772\n",
      "[Epoch: 30, Data seen: 40000] loss: 0.777\n",
      "Finished Training\n",
      "File saved\n"
     ]
    }
   ],
   "source": [
    "model_path = run_CNN(epochs=30, save=True, learning_rate=0.01, data=new_data, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d94dda",
   "metadata": {},
   "source": [
    "### Accuracy of model trained on expanded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fef50afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class airplane  Precision: 68.80 % Recall: 40.76 % F-Score: 51.19\n",
      "Class automobile  Precision: 74.80 % Recall: 42.79 % F-Score: 54.44\n",
      "Class bird   Precision: 41.30 % Recall: 29.23 % F-Score: 34.23\n",
      "Class cat    Precision: 45.90 % Recall: 31.46 % F-Score: 37.33\n",
      "Class deer   Precision: 59.30 % Recall: 37.23 % F-Score: 45.74\n",
      "Class dog    Precision: 51.60 % Recall: 34.04 % F-Score: 41.02\n",
      "Class frog   Precision: 76.70 % Recall: 43.41 % F-Score: 55.44\n",
      "Class horse  Precision: 66.20 % Recall: 39.83 % F-Score: 49.74\n",
      "Class ship   Precision: 74.20 % Recall: 42.59 % F-Score: 54.12\n",
      "Class truck  Precision: 76.60 % Recall: 43.37 % F-Score: 55.39\n",
      "\n",
      "Accuracy: 63.54 %\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_model(\"Augmented Data Accuracy per epoch\", 30, augmented=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93092dab",
   "metadata": {},
   "source": [
    " ## Required functions for activation maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "11bf42c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Char but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-180-e597da254bf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# perturb the grey image and try to maximize the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mtuned_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation_maximization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;31m# show the class activation map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-180-e597da254bf5>\u001b[0m in \u001b[0;36mactivation_maximization\u001b[0;34m(model, image, target, labels, num_iter, learning_rate)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# get the activation results of 10 output neurons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# get the activation of the target class neuron\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-168-465f0548c4c8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# relu activation function converts negative values to 0 on conv1, conv2, fc1 and fc2.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# flatten input to a 1 d tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    417\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 419\u001b[0;31m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0m\u001b[1;32m    420\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Char but found Float"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1152x1008 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def activation_maximization(model, image, target, labels, num_iter=1, learning_rate=0.2):\n",
    "    model.eval() \n",
    "\n",
    "    # zero the gradients\n",
    "    model.zero_grad()\n",
    "\n",
    "    for i in range(num_iter):\n",
    "        # get the activation results of 10 output neurons\n",
    "        model.float()\n",
    "        activations = model(image)\n",
    "\n",
    "        # get the activation of the target class neuron\n",
    "        target_activation = activations[0, target]\n",
    "        target_activation.backward()\n",
    "\n",
    "        # update the image\n",
    "        image.data += learning_rate * image.grad\n",
    "\n",
    "        # zero the gradients\n",
    "        image.grad.zero_()\n",
    "\n",
    "    # return tuned image\n",
    "    return image\n",
    "base_img = torch.from_numpy(np.zeros(shape=(1, 3, 32, 32), dtype=np.byte))\n",
    "normalize = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))])\n",
    "\n",
    "# Normalize grey image\n",
    "\n",
    "# base_img[0] = normalize(base_img[0])\n",
    "                                \n",
    "\n",
    "\n",
    "\n",
    "# Show initial grey image.\n",
    "imshow(base_img[0])\n",
    "# Make a base for each class\n",
    "airplane = torch.max(torch.tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1)[1]\n",
    "automobile = torch.max(torch.tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]]), 1)[1]\n",
    "bird = torch.max(torch.tensor([[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 1)[1]\n",
    "cat = torch.max(torch.tensor([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 1)[1]\n",
    "deer = torch.max(torch.tensor([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]), 1)[1]\n",
    "dog = torch.max(torch.tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1)[1]\n",
    "frog = torch.max(torch.tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 1)[1]\n",
    "horse = torch.max(torch.tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1)[1]\n",
    "ship = torch.max(torch.tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 1)[1]\n",
    "truck = torch.max(torch.tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), 1)[1]\n",
    "classes = [airplane, automobile, bird, cat, deer,dog,frog, horse, ship, truck]\n",
    "\n",
    "\n",
    "# Load the model \n",
    "loaded_network = Network()\n",
    "loaded_network.load_state_dict(torch.load('TrainedModels/augmented_model_20.pt'))\n",
    "\n",
    "fig = plt.figure(figsize=(16, 14))\n",
    "rows = 3\n",
    "columns = 4\n",
    "idx = 1\n",
    "for cl in classes:\n",
    "    # get a new initial grey image tensor\n",
    "    image_tensor = base_img\n",
    "    \n",
    "    # perturb the grey image and try to maximize the\n",
    "    tuned_image = activation_maximization(loaded_network, image_tensor, cl, y_train, num_iter=30, learning_rate=0.01)\n",
    "    # show the class activation map\n",
    "    fig.add_subplot(rows, columns, idx)\n",
    "    show_image = inverse_normalize(tuned_image)\n",
    "    imshow(show_image, title='activation image forclass \\'' + labels[target] + '\\'')\n",
    "    idx += 1\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
