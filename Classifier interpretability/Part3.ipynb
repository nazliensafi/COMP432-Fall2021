{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "534cfae6",
   "metadata": {},
   "source": [
    "# 3. Classifier interpretability\n",
    "\n",
    "In this section we train two types of models: a decision tree, and a convolutional neural network, to inspect \"which model is more interpretable?\".\n",
    "\n",
    "In the following steps:\n",
    "\n",
    "1. Process the CIFAR-10 dataset.\n",
    "2. Define and train a convolutional neural network (CNN) classifier using PyTorch.\n",
    "3. Interpret the CNN using the 'activation maximization' technique.\n",
    "4. Define and train a decision tree classifier.\n",
    "5. Interpret the decision tree and\n",
    "\n",
    "**Run the code below** to import required packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e37ced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6556c701",
   "metadata": {},
   "source": [
    "Load data\n",
    "\n",
    "Srource: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df9232cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to datasets/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a91c084264f4c9b847ec5720cfcd1ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/cifar-10-python.tar.gz to datasets/\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def load_data(root='datasets/'):\n",
    "\n",
    "    # normalize values from [0, 1] to [-1, 1]\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    batch_size = 5\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(root=root, train=True,\n",
    "                                            download=True, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                              shuffle=True, num_workers=2)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root=root, train=False,\n",
    "                                           download=True, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                             shuffle=False, num_workers=2)    \n",
    "\n",
    "    \n",
    "    return trainloader, testloader\n",
    "\n",
    "\n",
    "\n",
    "# Get the train_oader and testloader\n",
    "trainloader, testloader = load_data()\n",
    "\n",
    "def unpickle(self, filepath, filename):\n",
    "    \"\"\"\n",
    "    read the file and return the file contents\n",
    "        \n",
    "    @param filepath: file relative path\n",
    "    @param filename: file name\n",
    "    \"\"\"\n",
    "    with open(os.path.join(filepath, filename), 'rb') as fo:\n",
    "        file = pickle.load(fo, encoding='bytes')\n",
    "    return file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4394db",
   "metadata": {},
   "source": [
    "### Define methods to read, unpickles the data batches and convert them into np arrays for training and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d42f3877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(filepath, filename):\n",
    "    \"\"\"\n",
    "    read the file and return the file contents\n",
    "    \n",
    "    @param filepath: file relative path\n",
    "    @param filename: file name\n",
    "    \"\"\"\n",
    "    with open(os.path.join(filepath, filename), 'rb') as fo:            \n",
    "        file = pickle.load(fo, encoding='bytes')\n",
    "    return file\n",
    "\n",
    "def get_train_test():\n",
    "\n",
    "    # the folder path where it stores the CIFAR-10 training batches files\n",
    "    data_path = 'datasets/cifar-10-batches-py'\n",
    "        \n",
    "    # file names for training batches\n",
    "    training_batches = ('data_batch_1', 'data_batch_2', 'data_batch_3', 'data_batch_4', 'data_batch_5')        \n",
    "       \n",
    "    # temporary variables for reading files\n",
    "    train_data = None\n",
    "    train_labels = None\n",
    "        \n",
    "    for filename in training_batches:\n",
    "        # read the file contents\n",
    "        dict_train = self.unpickle(data_path, filename)\n",
    "        file_data = dict_train[b'data']\n",
    "        file_labels = dict_train[b'labels']\n",
    "            \n",
    "        # append the file contents in each batch to train_data and train_labels\n",
    "        if train_data is None and train_labels is None:\n",
    "            # for the first training batch\n",
    "            train_data = file_data\n",
    "            train_labels = file_labels\n",
    "        else:\n",
    "            # for the following training batches\n",
    "            #Stack arrays in sequence vertically (row wise) using vstack\n",
    "            # This is equivalent to concatenation along the \n",
    "            # first axis after 1-D arrays of shape (N,) have been reshaped to (1,N). \n",
    "            train_data = np.vstack((train_data, file_data))\n",
    "            # stack arrays in sequence horizontally (column wise) using hstack\n",
    "            train_labels = np.hstack((train_labels, file_labels))\n",
    "    x_train = train_data      # (50000, 3072)\n",
    "    y_train = train_labels    # (50000,)\n",
    "    \n",
    "    # read test batch file\n",
    "    test_batch = 'test_batch'\n",
    "    \n",
    "    # read the file contents\n",
    "    dict_test = unpickle(data_path, test_batch)\n",
    "        \n",
    "    x_test = dict_test[b'data']      # (10000, 3072)\n",
    "    y_test = dict_test[b'labels']    # (10000,)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b339a7ac",
   "metadata": {},
   "source": [
    "Define, train and save a CNN classifier using any architecture you like, but keep trying until you achieve at least 75% accuracy on the CIFAR test images. Train using CrossEntropyLoss. \n",
    "\n",
    "## Define a Covolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b18fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7703f65e",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c29f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "617a1ec0",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8666be74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0feafb21",
   "metadata": {},
   "source": [
    "## Evaluate the CNN above on the test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2195759a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73358e33",
   "metadata": {},
   "source": [
    " ## Required functions for activation maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1648a6af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bb6883c",
   "metadata": {},
   "source": [
    "## Load an initial grey image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18502613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b8cff05",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dd3126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec0f6687",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecb349b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72e62b3f",
   "metadata": {},
   "source": [
    "## Define a Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1ffae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4056d721",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96473c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
