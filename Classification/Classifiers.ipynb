{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b9121b",
   "metadata": {},
   "source": [
    "In this notebook we evaluate each of the following classification models:\n",
    "\n",
    "1. Logistic regression (for classification)\n",
    "2. Support vector classification\n",
    "3. Decision tree classification\n",
    "4. Random forest classification\n",
    "5. k-nearest neighbours classification\n",
    "6. AdaBoost classification\n",
    "7. Gaussian naive Bayes classification\n",
    "8. Neural network classification\n",
    "\n",
    "**Run the code below** to import required packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e37ced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sklearn as sk\n",
    "from sklearn import linear_model, tree, svm, ensemble, neighbors, naive_bayes, neural_network\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, RandomizedSearchCV, ParameterGrid\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.io.arff import loadarff\n",
    "import pandas as pd\n",
    "import scipy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9f936c",
   "metadata": {},
   "source": [
    "### Data and Classifier Parameters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "576b339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relative path root for datasets\n",
    "file_loc = os.path.join(os.getcwd(),'Datasets/')\n",
    "\n",
    "#Dataset files and parameters based on descriptions\n",
    "dataset_details = {\n",
    "    'yeast': {\n",
    "        'file': ['yeast.data'],\n",
    "        'load_params': {\n",
    "            'index_col': 0,\n",
    "            'delim_whitespace': 'true'\n",
    "        }\n",
    "        }\n",
    "}\n",
    "\n",
    "# Classifiers used and their subset of hyperparameters chosen to test with gridsearch\n",
    "CLASSIFIERS = {\n",
    "    'logreg': {\n",
    "        'clf': linear_model.LogisticRegression,\n",
    "       'param_grid': {\n",
    "            'C' : [100, 10, 1.0, 0.1, 0.01],\n",
    "            'random_state': [0]\n",
    "        },\n",
    "        'params': {\n",
    "            'random_state': 0\n",
    "        }\n",
    "    },\n",
    "    'tree': {\n",
    "        'clf': tree.DecisionTreeClassifier,\n",
    "        'param_grid': {\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'max_depth': [5,10,20,50,100,None],\n",
    "            'random_state': [0]\n",
    "        },\n",
    "        'params': {\n",
    "            'random_state': 0\n",
    "        }\n",
    "    },\n",
    "    'kneighbors': {\n",
    "        'clf': neighbors.KNeighborsClassifier,\n",
    "        'param_grid': {\n",
    "            'leaf_size': [1,5,10,20,50],\n",
    "            'n_neighbors': [1,5,10,20,30],\n",
    "            'p': [1,2]\n",
    "        },\n",
    "        'params': {}\n",
    "    },\n",
    "    'adaboost': {\n",
    "        'clf': ensemble.AdaBoostClassifier,\n",
    "        'param_grid': {\n",
    "            'n_estimators': [10, 50, 100, 500],\n",
    "            'learning_rate': [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "            'random_state': [0]\n",
    "            },\n",
    "        'params': {\n",
    "            'random_state': 0\n",
    "        }\n",
    "    },\n",
    "    'forest': {\n",
    "        'clf': ensemble.RandomForestClassifier,\n",
    "        'param_grid': {\n",
    "            'max_depth': [5, 10, 20, 50, 100, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'n_estimators': [100, 500, 1000, 1500, 2000]\n",
    "        },\n",
    "        'params': {}\n",
    "    },  \n",
    "    'svc': {\n",
    "        'clf': svm.SVC, \n",
    "        'param_grid': {\n",
    "            'kernel': ['rbf'],\n",
    "            'C': [50, 10, 1.0, 0.1, 0.01]\n",
    "        },\n",
    "        'params': {}\n",
    "    },\n",
    "    'neural': {\n",
    "        'clf': neural_network.MLPClassifier,\n",
    "        'param_grid': {\n",
    "            'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "            'activation': ['tanh', 'relu'],\n",
    "            'solver': ['sgd', 'adam'],\n",
    "            'alpha': [0.0001, 0.05],\n",
    "            'learning_rate': ['constant','adaptive'],\n",
    "            'max_iter': [100]\n",
    "        },\n",
    "        'params': {\n",
    "            'max_iter': 100\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e2a72a",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4d6b76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(dataset_details, file_loc, strategy='mean', encoder=StandardScaler, gridsearchflag=True):\n",
    "\n",
    "    '''\n",
    "    Loads all datasets in\n",
    "    Standardizes to dataframe\n",
    "    Splits into train and testing data\n",
    "    '''\n",
    "\n",
    "    train_data = {}\n",
    "    test_data = {}\n",
    "\n",
    "    for dataset in dataset_details:\n",
    "        if (gridsearchflag==True):\n",
    "            if (dataset!='yeast'):\n",
    "                continue\n",
    "        X, y = load_dataset(dataset_details[dataset], file_loc)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "        #Encode categorical values and scale numeric values\n",
    "        X_enc, y_enc = create_encoders(X_train, y_train, encoder)\n",
    "        X_train = X_enc.transform(X_train)\n",
    "        X_test = X_enc.transform(X_test)\n",
    "        \n",
    "        if scipy.sparse.issparse(X_train):\n",
    "            X_train = X_train.toarray()\n",
    "            X_test = X_test.toarray()\n",
    "        if y_enc:\n",
    "            y_train = y_enc.transform(y_train)\n",
    "            y_test = y_enc.transform(y_test)\n",
    "\n",
    "        X_imp, X_train, X_test = impute(X_train, X_test, strategy)\n",
    "\n",
    "        train_data[dataset] = {\n",
    "            'X_train': X_train,\n",
    "            'y_train': y_train,\n",
    "        }\n",
    "        test_data[dataset] = {\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "            'X_imp': X_imp,\n",
    "            'X_enc': X_enc,\n",
    "            'y_enc': y_enc\n",
    "        }\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "def impute(X_train, X_test, strategy='mean'):\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy=strategy)\n",
    "    X_imp = imp.fit(X_train)\n",
    "    X_train = X_imp.transform(X_train)\n",
    "    X_test = X_imp.transform(X_test)\n",
    "\n",
    "    return X_imp, X_train, X_test\n",
    "\n",
    "def create_encoders(X, y, encoder):\n",
    "\n",
    "    '''\n",
    "    Splits dataset into numerical and categorical data\n",
    "    creates relevant encoders for both features and labels\n",
    "    '''\n",
    "\n",
    "    cat_enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    num_enc = encoder()\n",
    "\n",
    "    cat_features = X.select_dtypes(include=['object']).columns\n",
    "    num_features =X.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "    if len(cat_features)==0:\n",
    "        X_enc = num_enc\n",
    "    elif len(num_features)==0:\n",
    "        X_enc = cat_enc\n",
    "    else:\n",
    "        X_enc = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"num\", num_enc, num_features),\n",
    "                (\"cat\", cat_enc, cat_features)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    X_enc.fit(X)\n",
    "    \n",
    "    y_enc = None\n",
    "    if y.dtypes=='object':\n",
    "        y_enc = LabelEncoder().fit(y)\n",
    "\n",
    "\n",
    "    return X_enc, y_enc\n",
    "    \n",
    "def load_dataset(dataset, file_loc):\n",
    "    '''\n",
    "    Loads in a dataset according to type and load_params\n",
    "    Assumes dataset file is either .xls, .arff, or plain text\n",
    "    If test and train are pre-split, assumes they are the same file type and combines for preprocessing.\n",
    "    Separates out the last column as y\n",
    "    '''\n",
    "\n",
    "    #metadata = dataset_details[dataset]\n",
    "    filenames = dataset['file']\n",
    "    load_params = dataset['load_params']\n",
    "\n",
    "    dfs = []\n",
    "    for file in filenames:\n",
    "        extension = file.split('.')[1]  # Get file type\n",
    "        file = f'{file_loc}{file}'\n",
    "        if extension == 'xls':\n",
    "            df = load_excel(file, **load_params)\n",
    "        elif extension == 'arff':\n",
    "            df = load_arff(file)\n",
    "        else:\n",
    "            df = load_plaintext(file, **load_params)\n",
    "        dfs.append(df)\n",
    "        df = pd.concat(dfs)\n",
    "    \n",
    "    y = df.iloc[:,-1]\n",
    "    X = df = df.iloc[: , :-1]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def load_excel(file,  **kwargs):\n",
    "    df = pd.read_excel(file, dtype=None, engine='xlrd', **kwargs)\n",
    "    return df\n",
    "\n",
    "def load_arff(file):\n",
    "    data = loadarff(file)\n",
    "    df = pd.DataFrame(data[0])\n",
    "    return df\n",
    "\n",
    "def load_plaintext(file, **kwargs):\n",
    "    df = pd.read_csv(file, header=None, dtype=None, **kwargs)\n",
    "    return df\n",
    "\n",
    "train_data, test_data = preprocessor(dataset_details, file_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f66a57",
   "metadata": {},
   "source": [
    "### Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e741571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  logreg  on  yeast\n",
      "Training  tree  on  yeast\n",
      "Training  kneighbors  on  yeast\n",
      "Training  adaboost  on  yeast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\comp432\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "def train_classifiers(data, CLASSIFIERS):\n",
    "\n",
    "    '''\n",
    "    Trains every classifier on every dataset\n",
    "    '''\n",
    "\n",
    "    models = {}\n",
    "    for clf in CLASSIFIERS:\n",
    "        models[clf]={}\n",
    "        for dataset in data:\n",
    "            print(\"Training \",clf,\" on \",dataset)\n",
    "            model = train_clf(CLASSIFIERS[clf], data[dataset]['X_train'], data[dataset]['y_train'])\n",
    "            models[clf][dataset] = {\n",
    "                'model': model\n",
    "            }\n",
    "        \n",
    "\n",
    "    return models\n",
    "\n",
    "def train_clf(clf_data, X, y):\n",
    "\n",
    "    '''\n",
    "    Trains a given classifier on a given dataset\n",
    "    '''\n",
    "    params = clf_data['params']\n",
    "    clf = clf_data['clf']\n",
    "    model = clf(**params).fit(X,y)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "models = train_classifiers(train_data, CLASSIFIERS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987c2ab9",
   "metadata": {},
   "source": [
    "### Choose Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fea4593a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\comp432\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1.0, 'random_state': 0}\n",
      "{'max_depth': 5, 'min_samples_split': 10, 'random_state': 0}\n",
      "{'leaf_size': 1, 'n_neighbors': 10, 'p': 1}\n",
      "{'learning_rate': 1.0, 'n_estimators': 10, 'random_state': 0}\n"
     ]
    }
   ],
   "source": [
    "def find_hyperparams(model, classifier_details, X, y, gridsearch):\n",
    "\n",
    "    param_grid = classifier_details['param_grid']\n",
    "    cv = KFold(n_splits=5)\n",
    "    search = gridsearch(model, param_grid, cv=cv, n_jobs=-1)\n",
    "\n",
    "    result = search.fit(X, y)\n",
    "    print (result.best_params_)\n",
    "    return result.best_params_\n",
    "\n",
    "def find_all_hyperparams(data, models, classifiers, gridsearch=GridSearchCV, gridsearchflag=True):\n",
    "    count = 0\n",
    "    for clf in models:\n",
    "        for dataset in models[clf]:\n",
    "            if (gridsearchflag==True):\n",
    "                if (dataset!='yeast'): continue\n",
    "            hps = find_hyperparams(models[clf][dataset]['model'], classifiers[clf], data[dataset]['X_train'], data[dataset]['y_train'], gridsearch)\n",
    "            models[clf][dataset]['params'] = hps\n",
    "        count+=1\n",
    "\n",
    "\n",
    "find_all_hyperparams(train_data,models,CLASSIFIERS)\n",
    "\n",
    "#cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=0)\n",
    "#search = GridSearchCV(models['tree']['adult']['model'], param_grid, cv=cv, n_jobs=-1)\n",
    "\n",
    "#result = search.fit(train_data['adult']['X_train'], train_data['adult']['y_train'])\n",
    "#classifier_details['params']=result.best_params_\n",
    "#print (result.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083cb67f",
   "metadata": {},
   "source": [
    "### Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f795d4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            yeast\n",
      "logreg       0.59\n",
      "tree         0.51\n",
      "kneighbors   0.57\n",
      "adaboost     0.45\n",
      "forest       0.62\n",
      "svc          0.61\n",
      "neural       0.62\n"
     ]
    }
   ],
   "source": [
    "def test_classifiers(data, models, scoring=f1_score, gridsearchflag=False):\n",
    "    scores = {}\n",
    "\n",
    "    for clf in models:\n",
    "        scores[clf] = []\n",
    "        for dataset in models[clf]:\n",
    "            if (gridsearchflag==True):\n",
    "                if (dataset!='yeast'): continue\n",
    "            score = test_clf(models[clf][dataset], data[dataset])\n",
    "            scores[clf].append(score)\n",
    "\n",
    "    scores = pd.DataFrame.from_dict(scores,orient='index',columns=dataset_details.keys())\n",
    "    np.set_printoptions(edgeitems=3)\n",
    "    np.core.arrayprint._line_width = 100\n",
    "    print(scores)\n",
    "            \n",
    "\n",
    "def test_clf(models, test_data):\n",
    "    model = models['model']\n",
    "    X = test_data['X_test']\n",
    "    y = test_data['y_test']\n",
    "\n",
    "    X = test_data['X_imp'].transform(X)\n",
    "    \n",
    "    if scipy.sparse.issparse(X):\n",
    "        X = X.toarray()\n",
    "    if models.get('y_enc'):\n",
    "        y = models['y_enc'].transform(y)\n",
    "    \n",
    "    y_pred = model.predict(X)\n",
    "    f1 = f1_score(y,y_pred,average='micro')\n",
    "    \n",
    "    return round(f1,2)\n",
    "\n",
    "test_classifiers(test_data, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213de744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6769b57e",
   "metadata": {},
   "source": [
    "### Decision Gridsearch (Novelty Component)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373261cd",
   "metadata": {},
   "source": [
    "The aim of a \"decision gridsearch\" is to analyze the decisions made at every step of preprocessing, training and testing, to determine if what we as students thought made sense actually corresponds to the highest scores. While we can't try every decision or every possibility for a given decision, the goal is to give some insight as to what kind of decisions need to have more thought put into them and what kind have minimal impact on a given type of dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b7997e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean <class 'sklearn.preprocessing._data.MinMaxScaler'> <class 'sklearn.model_selection._search.GridSearchCV'> <function f1_score at 0x000002717C167B80>\n",
      "            yeast\n",
      "logreg       0.59\n",
      "tree         0.59\n",
      "kneighbors   0.58\n",
      "adaboost     0.44\n",
      "most_frequent <class 'sklearn.preprocessing._data.MinMaxScaler'> <class 'sklearn.model_selection._search.GridSearchCV'> <function f1_score at 0x000002717C167B80>\n",
      "            yeast\n",
      "logreg       0.59\n",
      "tree         0.59\n",
      "kneighbors   0.58\n",
      "adaboost     0.44\n",
      "mean <class 'sklearn.preprocessing._data.MinMaxScaler'> <class 'sklearn.model_selection._search.GridSearchCV'> score\n",
      "            yeast\n",
      "logreg       0.59\n",
      "tree         0.59\n",
      "kneighbors   0.58\n",
      "adaboost     0.44\n",
      "most_frequent <class 'sklearn.preprocessing._data.MinMaxScaler'> <class 'sklearn.model_selection._search.GridSearchCV'> score\n",
      "            yeast\n",
      "logreg       0.59\n",
      "tree         0.59\n",
      "kneighbors   0.58\n",
      "adaboost     0.44\n",
      "mean <class 'sklearn.preprocessing._data.StandardScaler'> <class 'sklearn.model_selection._search.GridSearchCV'> <function f1_score at 0x000002717C167B80>\n",
      "            yeast\n",
      "logreg       0.59\n",
      "tree         0.59\n",
      "kneighbors   0.58\n",
      "adaboost     0.44\n",
      "most_frequent <class 'sklearn.preprocessing._data.StandardScaler'> <class 'sklearn.model_selection._search.GridSearchCV'> <function f1_score at 0x000002717C167B80>\n",
      "            yeast\n",
      "logreg       0.59\n",
      "tree         0.59\n",
      "kneighbors   0.58\n",
      "adaboost     0.44\n",
      "mean <class 'sklearn.preprocessing._data.StandardScaler'> <class 'sklearn.model_selection._search.GridSearchCV'> score\n",
      "            yeast\n",
      "logreg       0.59\n",
      "tree         0.59\n",
      "kneighbors   0.58\n",
      "adaboost     0.44\n",
      "most_frequent <class 'sklearn.preprocessing._data.StandardScaler'> <class 'sklearn.model_selection._search.GridSearchCV'> score\n",
      "            yeast\n",
      "logreg       0.59\n",
      "tree         0.59\n",
      "kneighbors   0.58\n",
      "adaboost     0.44\n",
      "mean <class 'sklearn.preprocessing._data.MinMaxScaler'> <class 'sklearn.model_selection._search.RandomizedSearchCV'> <function f1_score at 0x000002717C167B80>\n",
      "            yeast\n",
      "logreg       0.59\n",
      "tree         0.59\n",
      "kneighbors   0.58\n",
      "adaboost     0.44\n",
      "most_frequent <class 'sklearn.preprocessing._data.MinMaxScaler'> <class 'sklearn.model_selection._search.RandomizedSearchCV'> <function f1_score at 0x000002717C167B80>\n",
      "            yeast\n",
      "logreg       0.59\n",
      "tree         0.59\n",
      "kneighbors   0.58\n",
      "adaboost     0.44\n",
      "mean <class 'sklearn.preprocessing._data.MinMaxScaler'> <class 'sklearn.model_selection._search.RandomizedSearchCV'> score\n",
      "            yeast\n",
      "logreg       0.59\n",
      "tree         0.59\n",
      "kneighbors   0.58\n",
      "adaboost     0.44\n",
      "most_frequent <class 'sklearn.preprocessing._data.MinMaxScaler'> <class 'sklearn.model_selection._search.RandomizedSearchCV'> score\n",
      "            yeast\n",
      "logreg       0.59\n",
      "tree         0.59\n",
      "kneighbors   0.58\n",
      "adaboost     0.44\n",
      "mean <class 'sklearn.preprocessing._data.StandardScaler'> <class 'sklearn.model_selection._search.RandomizedSearchCV'> <function f1_score at 0x000002717C167B80>\n",
      "            yeast\n",
      "logreg       0.59\n",
      "tree         0.59\n",
      "kneighbors   0.58\n",
      "adaboost     0.44\n",
      "most_frequent <class 'sklearn.preprocessing._data.StandardScaler'> <class 'sklearn.model_selection._search.RandomizedSearchCV'> <function f1_score at 0x000002717C167B80>\n",
      "            yeast\n",
      "logreg       0.59\n",
      "tree         0.59\n",
      "kneighbors   0.58\n",
      "adaboost     0.44\n",
      "mean <class 'sklearn.preprocessing._data.StandardScaler'> <class 'sklearn.model_selection._search.RandomizedSearchCV'> score\n",
      "            yeast\n",
      "logreg       0.59\n",
      "tree         0.59\n",
      "kneighbors   0.58\n",
      "adaboost     0.44\n",
      "most_frequent <class 'sklearn.preprocessing._data.StandardScaler'> <class 'sklearn.model_selection._search.RandomizedSearchCV'> score\n",
      "            yeast\n",
      "logreg       0.59\n",
      "tree         0.59\n",
      "kneighbors   0.58\n",
      "adaboost     0.44\n"
     ]
    }
   ],
   "source": [
    "example_dataset = 'yeast' #Chose a dataset that had particularly low scores to begin with \n",
    "\n",
    "decisions = {\n",
    "    'strategy': ['mean','most_frequent'],\n",
    "    'scaler': [MinMaxScaler, StandardScaler],\n",
    "    'gridsearch': [GridSearchCV, RandomizedSearchCV],\n",
    "    'scoring': [f1_score, 'score']\n",
    "}\n",
    "\n",
    "combinations = ParameterGrid(decisions)\n",
    "\n",
    "for combination in combinations:\n",
    "    strategy = combination['strategy']\n",
    "    scaler = combination['scaler']\n",
    "    gridsearch = combination['gridsearch']\n",
    "    scoring = combination['scoring']\n",
    "    print(strategy, scaler, gridsearch, scoring)\n",
    "    preprocessor(dataset_details, file_loc, strategy=strategy, encoder=scaler, gridsearchflag=True)\n",
    "    find_all_hyperparams(train_data, models, CLASSIFIERS, gridsearch=gridsearch, gridsearchflag=True)\n",
    "    test_classifiers(test_data, models, gridsearchflag=True)\n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
