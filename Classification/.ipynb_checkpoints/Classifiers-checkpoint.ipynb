{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b9121b",
   "metadata": {},
   "source": [
    "In this notebook we evaluate each of the following classification models:\n",
    "\n",
    "1. Logistic regression (for classification)\n",
    "2. Support vector classification\n",
    "3. Decision tree classification\n",
    "4. Random forest classification\n",
    "5. k-nearest neighbours classification\n",
    "6. AdaBoost classification\n",
    "7. Gaussian naive Bayes classification\n",
    "8. Neural network classification\n",
    "\n",
    "**Run the code below** to import required packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e37ced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sklearn as sk\n",
    "from sklearn import linear_model, tree, svm, ensemble, neighbors, naive_bayes, neural_network\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.io.arff import loadarff\n",
    "import pandas as pd\n",
    "import scipy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9f936c",
   "metadata": {},
   "source": [
    "### Data and Classifier Parameters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "576b339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relative path root for datasets\n",
    "file_loc = os.path.join(os.getcwd(),'Datasets/')\n",
    "\n",
    "#Dataset files and parameters based on descriptions\n",
    "dataset_details = {\n",
    "    'credit':\n",
    "        {\n",
    "            'file': ['default of credit card clients.xls'],\n",
    "            'load_params': {\n",
    "                'index_col': 0,\n",
    "                'skiprows': 1\n",
    "            }\n",
    "        },\n",
    "    'breast_cancer':\n",
    "        {\n",
    "            'file': ['breast-cancer-wisconsin.data'],\n",
    "            'load_params': {\n",
    "                'na_values': '?',\n",
    "                'index_col': 0\n",
    "            },\n",
    "            'weighted': 'Y'\n",
    "        },\n",
    "    'statlog':\n",
    "        {\n",
    "            'file': ['german.data-numeric'],\n",
    "            'load_params': {\n",
    "                'delim_whitespace': 'true'\n",
    "            },\n",
    "            'weighted': 'Y',\n",
    "        },\n",
    "    'adult':\n",
    "        {\n",
    "            'file': ['adult.data','adult.test'],\n",
    "            'load_params': {\n",
    "                'na_values': '?',\n",
    "                'comment': '|'\n",
    "            }\n",
    "        },\n",
    "    'yeast': {\n",
    "        'file': ['yeast.data'],\n",
    "        'load_params': {\n",
    "            'index_col': 0,\n",
    "            'delim_whitespace': 'true'\n",
    "        }\n",
    "    },\n",
    "    'thoracic': {\n",
    "        'file': ['ThoraricSurgery.arff'],\n",
    "        'load_params': {}\n",
    "    },\n",
    "    'seismic': {\n",
    "        'file': ['seismic-bumps.arff'],\n",
    "        'load_params': {}\n",
    "    },\n",
    "    'retinopathy': {\n",
    "        'file': ['messidor_features.arff'],\n",
    "        'load_params': {\n",
    "            'comment': '@'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Classifiers used and their subset of hyperparameters chosen to test with gridsearch\n",
    "CLASSIFIERS = {\n",
    "    'logreg': {\n",
    "        'clf': linear_model.LogisticRegression,\n",
    "       'param_grid': {\n",
    "            'C' : [100, 10, 1.0, 0.1, 0.01]\n",
    "        },\n",
    "        'params': {\n",
    "            'random_state': 0\n",
    "        }\n",
    "    },\n",
    "    'tree': {\n",
    "        'clf': tree.DecisionTreeClassifier,\n",
    "        'param_grid': {\n",
    "            'min_samples_split': [5,10,100,500],\n",
    "            'max_depth': [1,5,10,50,100]\n",
    "        },\n",
    "        'params': {\n",
    "            'random_state': 0\n",
    "        }\n",
    "    },\n",
    "    'kneighbors': {\n",
    "        'clf': neighbors.KNeighborsClassifier,\n",
    "        'param_grid': {\n",
    "            'leaf_size': [1,5,10,20,50],\n",
    "            'n_neighbors': [1,5,10,20,30],\n",
    "            'p': [1,2]\n",
    "        },\n",
    "        'params': {}\n",
    "    },\n",
    "    'adaboost': {\n",
    "        'clf': ensemble.AdaBoostClassifier,\n",
    "        'param_grid': {\n",
    "            'n_estimators': [10, 50, 100, 500],\n",
    "            'learning_rate': [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "            },\n",
    "        'params': {\n",
    "            'random_state': 0\n",
    "        }\n",
    "    },\n",
    "    'nb': {\n",
    "        'clf': naive_bayes.GaussianNB,\n",
    "        'param_grid': {},        \n",
    "        'params': {}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e2a72a",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4d6b76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(dataset_details, file_loc):\n",
    "\n",
    "    '''\n",
    "    Loads all datasets in\n",
    "    Standardizes to dataframe\n",
    "    Splits into train and testing data\n",
    "    '''\n",
    "\n",
    "    train_data = {}\n",
    "    test_data = {}\n",
    "\n",
    "    for dataset in dataset_details:\n",
    "        X, y = load_dataset(dataset_details[dataset], file_loc)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "        #if dataset=='breast_cancer': #train_test_split is converting all 10.0 values to NaN for this dataset for some reason\n",
    "            #X_train = X_train.fillna(value=10)\n",
    "            #X_test = X_test.fillna(value=10)\n",
    "        \n",
    "        \n",
    "        #Encode categorical values and scale numeric values\n",
    "        X_enc, y_enc = create_encoders(X_train, y_train)\n",
    "        X_train = X_enc.transform(X_train)\n",
    "        X_test = X_enc.transform(X_test)\n",
    "        \n",
    "        if scipy.sparse.issparse(X_train):\n",
    "            X_train = X_train.toarray()\n",
    "            X_test = X_test.toarray()\n",
    "        if y_enc:\n",
    "            y_train = y_enc.transform(y_train)\n",
    "            y_test = y_enc.transform(y_test)\n",
    "\n",
    "            \n",
    "        #Impute missing values  \n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "        X_imp = imp.fit(X_train)\n",
    "        X_train = X_imp.transform(X_train)\n",
    "        X_test = X_imp.transform(X_test)\n",
    "        \n",
    "        train_data[dataset] = {\n",
    "            'X_train': X_train,\n",
    "            'y_train': y_train\n",
    "        }\n",
    "        test_data[dataset] = {\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test\n",
    "        }\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "def create_encoders(X, y):\n",
    "\n",
    "    '''\n",
    "    Splits dataset into numerical and categorical data\n",
    "    creates relevant encoders for both features and labels\n",
    "    '''\n",
    "\n",
    "    cat_enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    num_enc = StandardScaler()\n",
    "\n",
    "    cat_features = X.select_dtypes(include=['object']).columns\n",
    "    num_features =X.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "    if len(cat_features)==0:\n",
    "        X_enc = num_enc\n",
    "    elif len(num_features)==0:\n",
    "        X_enc = cat_enc\n",
    "    else:\n",
    "        X_enc = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"num\", num_enc, num_features),\n",
    "                (\"cat\", cat_enc, cat_features)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    X_enc.fit(X)\n",
    "    \n",
    "    y_enc = None\n",
    "    if y.dtypes=='object':\n",
    "        y_enc = LabelEncoder().fit(y)\n",
    "\n",
    "\n",
    "    return X_enc, y_enc\n",
    "    \n",
    "def load_dataset(dataset, file_loc):\n",
    "    '''\n",
    "    Loads in a dataset according to type and load_params\n",
    "    Assumes dataset file is either .xls, .arff, or plain text\n",
    "    If test and train are pre-split, assumes they are the same file type and combines for preprocessing.\n",
    "    Separates out the last column as y\n",
    "    '''\n",
    "\n",
    "    #metadata = dataset_details[dataset]\n",
    "    filenames = dataset['file']\n",
    "    load_params = dataset['load_params']\n",
    "\n",
    "    dfs = []\n",
    "    for file in filenames:\n",
    "        extension = file.split('.')[1]  # Get file type\n",
    "        file = f'{file_loc}{file}'\n",
    "        if extension == 'xls':\n",
    "            df = load_excel(file, **load_params)\n",
    "        elif extension == 'arff':\n",
    "            df = load_arff(file)\n",
    "        else:\n",
    "            df = load_plaintext(file, **load_params)\n",
    "        dfs.append(df)\n",
    "        df = pd.concat(dfs)\n",
    "    \n",
    "    y = df.iloc[:,-1]\n",
    "    X = df = df.iloc[: , :-1]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def load_excel(file,  **kwargs):\n",
    "    df = pd.read_excel(file, dtype=None, engine='xlrd', **kwargs)\n",
    "    return df\n",
    "\n",
    "def load_arff(file):\n",
    "    data = loadarff(file)\n",
    "    df = pd.DataFrame(data[0])\n",
    "    return df\n",
    "\n",
    "def load_plaintext(file, **kwargs):\n",
    "    df = pd.read_csv(file, header=None, dtype=None, **kwargs)\n",
    "    return df\n",
    "\n",
    "train_data, test_data = preprocessor(dataset_details, file_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f66a57",
   "metadata": {},
   "source": [
    "### Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e741571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  tree  on  credit\n",
      "Training  tree  on  breast_cancer\n",
      "Training  tree  on  statlog\n",
      "Training  tree  on  adult\n",
      "Training  tree  on  yeast\n",
      "Training  tree  on  thoracic\n",
      "Training  tree  on  seismic\n",
      "Training  tree  on  retinopathy\n",
      "Training  kneighbors  on  credit\n",
      "Training  kneighbors  on  breast_cancer\n",
      "Training  kneighbors  on  statlog\n",
      "Training  kneighbors  on  adult\n",
      "Training  kneighbors  on  yeast\n",
      "Training  kneighbors  on  thoracic\n",
      "Training  kneighbors  on  seismic\n",
      "Training  kneighbors  on  retinopathy\n",
      "Training  adaboost  on  credit\n",
      "Training  adaboost  on  breast_cancer\n",
      "Training  adaboost  on  statlog\n",
      "Training  adaboost  on  adult\n",
      "Training  adaboost  on  yeast\n",
      "Training  adaboost  on  thoracic\n",
      "Training  adaboost  on  seismic\n",
      "Training  adaboost  on  retinopathy\n",
      "Training  nb  on  credit\n",
      "Training  nb  on  breast_cancer\n",
      "Training  nb  on  statlog\n",
      "Training  nb  on  adult\n",
      "Training  nb  on  yeast\n",
      "Training  nb  on  thoracic\n",
      "Training  nb  on  seismic\n",
      "Training  nb  on  retinopathy\n"
     ]
    }
   ],
   "source": [
    "def train_classifiers(data, CLASSIFIERS):\n",
    "\n",
    "    '''\n",
    "    Trains every classifier on every dataset\n",
    "    '''\n",
    "\n",
    "    models = {}\n",
    "    for clf in CLASSIFIERS:\n",
    "        models[clf]={}\n",
    "        for dataset in data:\n",
    "            print(\"Training \",clf,\" on \",dataset)\n",
    "            model, X_enc, y_enc, X_imp = train_clf(CLASSIFIERS[clf], data[dataset]['X_train'], data[dataset]['y_train'])\n",
    "            models[clf][dataset] = {\n",
    "                'model': model,\n",
    "                'X_enc': X_enc,\n",
    "                'y_enc': y_enc,\n",
    "                'X_imp': X_imp\n",
    "            }\n",
    "        \n",
    "\n",
    "    return models\n",
    "\n",
    "def train_clf(clf_data, X, y):\n",
    "\n",
    "    '''\n",
    "    Trains a given classifier on a given dataset\n",
    "    '''\n",
    "    params = clf_data['params']\n",
    "    clf = clf_data['clf']\n",
    "    model = clf(**params).fit(X,y)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "models = train_classifiers(train_data, CLASSIFIERS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987c2ab9",
   "metadata": {},
   "source": [
    "### Choose Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fea4593a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11664/4038498460.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m#classifier_details['params']=result.best_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m#print (result.best_params_)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "def find_hyperparams(model, classifier_details, X, y):\n",
    "\n",
    "    param_grid = classifier_details['param_grid']\n",
    "    cv = KFold(n_splits=5)\n",
    "    search = GridSearchCV(model, param_grid, cv=cv, n_jobs=-1)\n",
    "\n",
    "    result = search.fit(X, y)\n",
    "    print (result.best_params_)\n",
    "    return result.best_params_\n",
    "\n",
    "def find_all_hyperparams(data, models, classifiers):\n",
    "    count = 0\n",
    "    for clf in models:\n",
    "        if count == 0:\n",
    "            count+=1\n",
    "            continue\n",
    "        for dataset in models[clf]:\n",
    "            if (dataset=='adult'):\n",
    "                continue\n",
    "            hps = find_hyperparams(models[clf][dataset]['model'], classifiers[clf], data[dataset]['X_train'], data[dataset]['y_train'])\n",
    "            models[clf][dataset]['params'] = hps\n",
    "\n",
    "\n",
    "#find_all_hyperparams(train_data,models,CLASSIFIERS)\n",
    "\n",
    "#cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=0)\n",
    "#search = GridSearchCV(models['tree']['adult']['model'], param_grid, cv=cv, n_jobs=-1)\n",
    "\n",
    "#result = search.fit(train_data['adult']['X_train'], train_data['adult']['y_train'])\n",
    "#classifier_details['params']=result.best_params_\n",
    "#print (result.best_params_)\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083cb67f",
   "metadata": {},
   "source": [
    "### Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f795d4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            credit  breast_cancer  statlog  adult  yeast  thoracic  seismic  \\\n",
      "logreg        0.81           0.97     0.76   0.57   0.56      0.81     0.95   \n",
      "tree          0.73           0.94     0.70   0.46   0.50      0.81     0.88   \n",
      "kneighbors    0.79           0.98     0.69   0.50   0.55      0.82     0.94   \n",
      "adaboost      0.82           0.96     0.78   0.57   0.45      0.81     0.94   \n",
      "nb            0.64           0.95     0.69   0.18   0.11      0.19     0.41   \n",
      "\n",
      "            retinopathy  \n",
      "logreg             0.66  \n",
      "tree               0.64  \n",
      "kneighbors         0.64  \n",
      "adaboost           0.67  \n",
      "nb                 0.63  \n"
     ]
    }
   ],
   "source": [
    "def test_classifiers(data, models):\n",
    "    scores = {}\n",
    "\n",
    "    for clf in models:\n",
    "        scores[clf] = []\n",
    "        for dataset in models[clf]:\n",
    "            f1_score = test_clf(models[clf][dataset], data[dataset]['X_test'], data[dataset]['y_test'])\n",
    "            scores[clf].append(f1_score)\n",
    "\n",
    "    scores = pd.DataFrame.from_dict(scores,orient='index',columns=dataset_details.keys())\n",
    "    np.set_printoptions(edgeitems=3)\n",
    "    np.core.arrayprint._line_width = 100\n",
    "    print(scores)\n",
    "            \n",
    "\n",
    "def test_clf(models, X, y):\n",
    "    model = models['model']\n",
    "    X = models['X_enc'].transform(X)\n",
    "    X = models['X_imp'].transform(X)\n",
    "    \n",
    "    if scipy.sparse.issparse(X):\n",
    "        X = X.toarray()\n",
    "    if models.get('y_enc'):\n",
    "        y = models['y_enc'].transform(y)\n",
    "    \n",
    "    y_pred = model.predict(X)\n",
    "    f1 = f1_score(y,y_pred,average='micro')\n",
    "    \n",
    "    return round(f1,2)\n",
    "\n",
    "test_classifiers(test_data, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6769b57e",
   "metadata": {},
   "source": [
    "### Decision Gridsearch (Novelty Component)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373261cd",
   "metadata": {},
   "source": [
    "The aim of a \"decision gridsearch\" is to analyze the decisions made at every step of preprocessing, training and testing, to determine if what we as students thought made sense actually corresponds to the highest scores. While we can't try every decision or every possibility for a given decision, the goal is to give some insight as to what kind of decisions need to have more thought put into them and what kind have minimal impact on a given type of dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7997e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dataset = 'yeast' #Chose a dataset that had particularly low scores to begin with \n",
    "\n",
    "decisions = {\n",
    "    'strategy': ['mean','most frequent'],\n",
    "    'scaler': [MinMaxScaler, StandardScaler],\n",
    "    'gridsearch': [GridSearchCV, RandomizedSearchCV],\n",
    "    'scoring': [f1_score, 'score']\n",
    "}\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
